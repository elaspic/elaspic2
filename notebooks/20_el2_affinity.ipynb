{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "Run EL2 to calculate affinity for rows in the ELASPIC database.\n",
    "\n",
    "### Executing\n",
    "\n",
    "```bash\n",
    "export NOTEBOOK_PATH=\"$(realpath 20_el2_affinity.ipynb)\"\n",
    "export DATASET_NAME=\"elaspic-interface-mutation-local\"\n",
    "export ORIGINAL_ARRAY_TASK_COUNT=9\n",
    "sbatch --export=DATASET_NAME,NOTEBOOK_PATH,ORIGINAL_ARRAY_TASK_COUNT --array=1-9 --ntasks-per-node=40 --mem=0 ../scripts/run_notebook_cpu.sh\n",
    "\n",
    "export NOTEBOOK_PATH=\"$(realpath 20_el2_affinity.ipynb)\"\n",
    "export DATASET_NAME=\"uniprot-domain-pair-mutation\"\n",
    "export ORIGINAL_ARRAY_TASK_COUNT=1358\n",
    "sbatch --export=DATASET_NAME,NOTEBOOK_PATH,ORIGINAL_ARRAY_TASK_COUNT --array=1-1358 --ntasks-per-node=48 ../scripts/run_notebook_cpu.sh\n",
    "\n",
    "# On Cedar\n",
    "--ntasks-per-node=48\n",
    " \n",
    "# On Niagara,\n",
    "--ntasks-per-node=40 --mem=0\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import socket\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "import elaspic2 as el2\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from kmbio import PDB\n",
    "from kmtools import structure_tools\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTEBOOK_DIR = Path(\"20_el2_affinity\").resolve()\n",
    "NOTEBOOK_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "NOTEBOOK_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"DATAPKG_OUTPUT_DIR\" in os.environ:\n",
    "    OUTPUT_DIR = Path(os.getenv(\"DATAPKG_OUTPUT_DIR\")).joinpath(\"elaspic2\").resolve()\n",
    "else:\n",
    "    OUTPUT_DIR = NOTEBOOK_DIR.parent\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (slurm_tmpdir := os.getenv(\"SLURM_TMPDIR\")) is not None:\n",
    "    os.environ[\"TMPDIR\"] = slurm_tmpdir\n",
    "    \n",
    "print(tempfile.gettempdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"scinet\" in socket.gethostname():\n",
    "    CPU_COUNT = 40\n",
    "else:\n",
    "    CPU_COUNT = max(1, len(os.sched_getaffinity(0)))\n",
    "\n",
    "CPU_COUNT = max(1, CPU_COUNT // 2)\n",
    "\n",
    "CPU_COUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = os.getenv(\"DATASET_NAME\")\n",
    "TASK_ID = os.getenv(\"SLURM_ARRAY_TASK_ID\")\n",
    "TASK_COUNT = os.getenv(\"ORIGINAL_ARRAY_TASK_COUNT\") or os.getenv(\"SLURM_ARRAY_TASK_COUNT\")\n",
    "TASK_ID_OFFSET = os.getenv(\"SLURM_TASK_ID_OFFSET\")\n",
    "\n",
    "TASK_ID = int(TASK_ID) if TASK_ID is not None else None\n",
    "TASK_COUNT = int(TASK_COUNT) if TASK_COUNT is not None else None\n",
    "TASK_ID_OFFSET = int(TASK_ID_OFFSET) if TASK_ID_OFFSET is not None else 0\n",
    "\n",
    "TASK_ID += TASK_ID_OFFSET\n",
    "\n",
    "DATASET_NAME, TASK_ID, TASK_COUNT, TASK_ID_OFFSET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = TASK_ID is None\n",
    "\n",
    "if DEBUG:\n",
    "    DATASET_NAME = \"uniprot-domain-pair-mutation\"\n",
    "    TASK_ID = 50\n",
    "    TASK_COUNT = 1358\n",
    "else:\n",
    "    assert DATASET_NAME is not None\n",
    "    assert TASK_ID is not None\n",
    "    assert TASK_COUNT is not None\n",
    "\n",
    "DATASET_NAME, TASK_ID, TASK_COUNT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = OUTPUT_DIR.joinpath(\n",
    "    \"..\", \"elaspic-data\", \"12_el2_to_recalculate\", f\"{DATASET_NAME}.parquet\"\n",
    ").resolve(strict=True)\n",
    "\n",
    "input_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfile = pq.ParquetFile(input_file)\n",
    "\n",
    "pfile.num_row_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert TASK_COUNT == pfile.num_row_groups, (TASK_COUNT, pfile.num_row_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DF = pfile.read_row_group(TASK_ID - 1).to_pandas(integer_object_nulls=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(INPUT_DF.head(2))\n",
    "print(len(INPUT_DF))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = el2.ELASPIC2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_mutation_to_chain(structure, chain_id, mutation):\n",
    "    df = structure.to_dataframe()\n",
    "    chain_df = df[df[\"chain_id\"] == chain_id]\n",
    "\n",
    "    residue_idx_corrected = chain_df[\"residue_idx\"] - chain_df[\"residue_idx\"].min()\n",
    "    residue_idx_map = {\n",
    "        old_residue_idx: new_residue_idx\n",
    "        for (old_residue_idx, new_residue_idx) in zip(\n",
    "            chain_df[\"residue_idx\"], residue_idx_corrected\n",
    "        )\n",
    "    }\n",
    "\n",
    "    pos = int(mutation[1:-1])\n",
    "    pos_new = residue_idx_map[pos - 1] + 1\n",
    "\n",
    "    wt = mutation[0]\n",
    "    mut = mutation[-1]\n",
    "    return f\"{wt}{pos_new}{mut}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_row(tup):\n",
    "    if not tup.structure.strip():\n",
    "        return None\n",
    "\n",
    "    with tempfile.NamedTemporaryFile(suffix=\".pdb\") as structure_file_obj:\n",
    "        with open(structure_file_obj.name, \"wt\") as fout:\n",
    "            fout.write(tup.structure)\n",
    "        structure = PDB.load(structure_file_obj.name)\n",
    "        protein_sequence = structure_tools.get_chain_sequence(\n",
    "            structure[0][tup.chain_modeller], if_unknown=\"replace\", unknown_residue_marker=\"\"\n",
    "        )\n",
    "\n",
    "        mutation = map_mutation_to_chain(structure, tup.chain_modeller, tup.mutation_modeller)\n",
    "        wt_aa = mutation[0]\n",
    "        pos = int(mutation[1:-1])\n",
    "\n",
    "        if len(protein_sequence) < pos or protein_sequence[pos - 1] != wt_aa:\n",
    "            print(f\"Protein sequence does not match mutation\")\n",
    "            return None\n",
    "\n",
    "        ligand_sequence = \"\"\n",
    "        for chain in structure[0].chains:\n",
    "            if chain.id == tup.chain_modeller:\n",
    "                continue\n",
    "            ligand_sequence = structure_tools.get_chain_sequence(\n",
    "                structure[0][chain.id], if_unknown=\"replace\", unknown_residue_marker=\"\"\n",
    "            )\n",
    "            if ligand_sequence:\n",
    "                break\n",
    "        if not ligand_sequence:\n",
    "            print(f\"Skipping row with no ligand sequence: {tup._replace(structure='')}\")\n",
    "            return None\n",
    "\n",
    "        protein_stability_features = model.build(\n",
    "            structure_file=structure_file_obj.name,\n",
    "            protein_sequence=protein_sequence,\n",
    "            ligand_sequence=None,\n",
    "            remove_hetatms=True,\n",
    "        )\n",
    "        protein_affinity_features = model.build(\n",
    "            structure_file=structure_file_obj.name,\n",
    "            protein_sequence=protein_sequence,\n",
    "            ligand_sequence=ligand_sequence,\n",
    "            remove_hetatms=True,\n",
    "        )\n",
    "    mutation_stability_features = model.analyze_mutation(mutation, protein_stability_features)\n",
    "    mutation_affinity_features = model.analyze_mutation(mutation, protein_affinity_features)\n",
    "\n",
    "    # Get final predictions\n",
    "    row = tup._asdict()\n",
    "    del row[\"Index\"], row[\"model_filename_wt\"], row[\"structure\"]\n",
    "\n",
    "    row[\"protbert_score\"] = (\n",
    "        mutation_affinity_features[\"protbert_interface_score_wt\"]\n",
    "        - mutation_affinity_features[\"protbert_interface_score_mut\"]\n",
    "    )\n",
    "    row[\"proteinsolver_score\"] = mutation_affinity_features[\"proteinsolver_interface_score_wt\"]\n",
    "    row[\"el2_score\"] = model.predict_mutation_effect(\n",
    "        [mutation_stability_features], [mutation_affinity_features]\n",
    "    ).item()\n",
    "\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HAJ2IpvMHD2f",
    "outputId": "47fee7a9-3581-48ce-c5cb-19d12b50972b"
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "for tup in tqdm(INPUT_DF.itertuples(), total=len(INPUT_DF)):\n",
    "    try:\n",
    "        row = prepare_row(tup)\n",
    "    except Exception as e:\n",
    "        print(f\"Encountered an error: {e}\")\n",
    "    else:\n",
    "        if row is not None:\n",
    "            results.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = OUTPUT_DIR.joinpath(\n",
    "    NOTEBOOK_DIR.name, DATASET_NAME, f\"{DATASET_NAME}-{TASK_ID:04d}-{TASK_COUNT:04d}.parquet\"\n",
    ")\n",
    "output_file.parent.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pq.write_table(pa.Table.from_pandas(results_df, preserve_index=False), output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with output_file.with_suffix(\".SUCCESS\").open(\"w\") as fout:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
