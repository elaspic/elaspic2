{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import crc32c\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTEBOOK_NAME = \"01_load_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTEBOOK_DIR = Path(NOTEBOOK_NAME).resolve()\n",
    "NOTEBOOK_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "NOTEBOOK_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"DATAPKG_OUTPUT_DIR\" in os.environ:\n",
    "    DATAPKG_OUTPUT_DIR = Path(os.getenv(\"DATAPKG_OUTPUT_DIR\")).resolve()\n",
    "else:\n",
    "    DATAPKG_OUTPUT_DIR = NOTEBOOK_DIR\n",
    "DATAPKG_OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "DATAPKG_OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"DATAPKG_OUTPUT_DIR\" in os.environ:\n",
    "    OUTPUT_DIR = Path(os.getenv(\"DATAPKG_OUTPUT_DIR\")).joinpath(\"elaspic-v2\").resolve()\n",
    "else:\n",
    "    OUTPUT_DIR = NOTEBOOK_DIR.parent\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resources = {\n",
    "    # === Core ===\n",
    "    \"elaspic-training-set-core\": DATAPKG_OUTPUT_DIR.joinpath(\n",
    "        \"elaspic-training-set\", \"02_export_data_core\", \"elaspic-training-set-core.parquet\"\n",
    "    ),\n",
    "    \"protherm-dagger-core\": DATAPKG_OUTPUT_DIR.joinpath(\n",
    "        \"protein-folding-energy\", \"protherm_dagger\", \"mutation-by-sequence.parquet\"\n",
    "    ),\n",
    "    \"rocklin-2017-core\": DATAPKG_OUTPUT_DIR.joinpath(\n",
    "        \"protein-folding-energy\", \"rocklin_2017\", \"mutation-ssm2.parquet\"\n",
    "    ),\n",
    "    # \"dunham-2020-core\": DATAPKG_OUTPUT_DIR.joinpath(\n",
    "    #     \"protein-folding-energy\", \"dunham_2020\", \"dunham-core.parquet\"\n",
    "    # ),\n",
    "    # === Interface ===\n",
    "    \"elaspic-training-set-interface\": DATAPKG_OUTPUT_DIR.joinpath(\n",
    "        \"elaspic-training-set\", \"02_export_data_interface\", \"elaspic-training-set-interface.parquet\"\n",
    "    ),\n",
    "    \"skempi-v2-interface\": DATAPKG_OUTPUT_DIR.joinpath(\n",
    "        \"protein-folding-energy\", \"skempi_v2\", \"skempi-v2.parquet\"\n",
    "    ),\n",
    "    # \"intact-mutations-interface\": DATAPKG_OUTPUT_DIR.joinpath(\n",
    "    #     \"protein-folding-energy\", \"intact_mutations\", \"intact-mutations.parquet\"\n",
    "    # ),\n",
    "    # \"dunham-2020-interface\": DATAPKG_OUTPUT_DIR.joinpath(\n",
    "    #     \"protein-folding-energy\", \"dunham_2020\", \"dunham-interface.parquet\"\n",
    "    # ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, path in resources.items():\n",
    "    assert Path(path).is_file(), path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "    \"unique_id\",\n",
    "    \"dataset\",\n",
    "    \"name\",\n",
    "    \"protein_sequence\",\n",
    "    \"ligand_sequence\",\n",
    "    \"mutation\",\n",
    "    \"effect\",\n",
    "    \"effect_type\",\n",
    "    \"protein_structure\",\n",
    "]\n",
    "\n",
    "extra_columns = [\n",
    "    \"provean_score\",\n",
    "    \"foldx_score\",\n",
    "    \"elaspic_score\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_id(dataset, effect_type, protein_sequence, ligand_sequence):\n",
    "    if ligand_sequence is not None:\n",
    "        key = f\"{dataset}|{effect_type}|{protein_sequence}|{ligand_sequence}\"\n",
    "    else:\n",
    "        key = f\"{dataset}|{effect_type}|{protein_sequence}\"\n",
    "    return crc32c.crc32c(key.encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = OUTPUT_DIR.joinpath(NOTEBOOK_NAME).resolve()\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_seen = {\n",
    "    \"core\": set(),\n",
    "    \"interface\": set(),\n",
    "}\n",
    "\n",
    "for dataset_name, dataset_file in resources.items():\n",
    "    coi = dataset_name.rsplit(\"-\", 1)[-1]\n",
    "    assert coi in [\"core\", \"interface\"]\n",
    "\n",
    "    df = (\n",
    "        pq.read_table(dataset_file)\n",
    "        .to_pandas(integer_object_nulls=True)\n",
    "        .rename(columns={\"mutations\": \"mutation\"})\n",
    "    )\n",
    "    print(f\"Read {len(df)} rows.\")\n",
    "\n",
    "    # Remove unneeded data\n",
    "    mask = df[\"mutation\"].apply(len) >= 2\n",
    "    print(f\"Removing {(~mask).sum()} rows with fewer than two mutations.\")\n",
    "    df = df[mask]\n",
    "\n",
    "    mask = df[\"effect\"].apply(lambda x: len(set(x))) >= 2\n",
    "    print(f\"Removing {(~mask).sum()} rows with fewer than two unique effects.\")\n",
    "    df = df[mask]\n",
    "\n",
    "    if \"dataset\" not in df:\n",
    "        df[\"dataset\"] = dataset_name\n",
    "    if \"ligand_sequence\" not in df:\n",
    "        df[\"ligand_sequence\"] = None\n",
    "\n",
    "    # Add a unique id\n",
    "    df[\"unique_id\"] = [\n",
    "        get_unique_id(dataset, effect_type, protein_sequence, ligand_sequence)\n",
    "        for dataset, effect_type, protein_sequence, ligand_sequence in df[\n",
    "            [\"dataset\", \"effect_type\", \"protein_sequence\", \"ligand_sequence\"]\n",
    "        ].values\n",
    "    ]\n",
    "    unique_ids = set(df[\"unique_id\"].values)\n",
    "    assert len(unique_ids) == len(df)\n",
    "    assert not set(unique_ids) & _seen[coi]\n",
    "    _seen[coi].update(unique_ids)\n",
    "\n",
    "    columns_all = columns + [c for c in extra_columns if c in df]\n",
    "    df_out = df[columns_all]\n",
    "\n",
    "    # Write files\n",
    "    row_per_batch = 100\n",
    "    writer = None\n",
    "    for start in tqdm(range(0, len(df_out), row_per_batch), desc=dataset_name):\n",
    "        stop = start + row_per_batch\n",
    "        df_out_slice = df_out.iloc[start:stop]\n",
    "        table = pa.Table.from_pandas(df_out_slice, preserve_index=False)\n",
    "\n",
    "        if writer is None:\n",
    "            output_file = output_dir.joinpath(f\"{dataset_name}.parquet\")\n",
    "            writer = pq.ParquetWriter(output_file, table.schema)\n",
    "            print(output_file)\n",
    "\n",
    "        writer.write_table(table)\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_seen = {\n",
    "    \"core\": set(),\n",
    "    \"interface\": set(),\n",
    "}\n",
    "\n",
    "for dataset_name, dataset_file in resources.items():\n",
    "    coi = dataset_name.rsplit(\"-\", 1)[-1]\n",
    "    assert coi in [\"core\", \"interface\"]\n",
    "\n",
    "    pfile = pq.ParquetFile(dataset_file)\n",
    "    writer = None\n",
    "    columns_all = None\n",
    "    for row_group_idx in tqdm(range(pfile.num_row_groups), desc=dataset_name):\n",
    "        df = (\n",
    "            pfile.read_row_group(row_group_idx)\n",
    "            .to_pandas(integer_object_nulls=True)\n",
    "            .rename(columns={\"mutations\": \"mutation\"})\n",
    "        )\n",
    "\n",
    "        # Remove unneeded data\n",
    "        mask = df[\"mutation\"].apply(len) >= 2\n",
    "        print(f\"Removing {(~mask).sum()} rows with fewer than two mutations.\")\n",
    "        df = df[mask]\n",
    "\n",
    "        mask = df[\"effect\"].apply(lambda x: len(set(x))) >= 2\n",
    "        print(f\"Removing {(~mask).sum()} rows with fewer than two unique effects.\")\n",
    "        df = df[mask]\n",
    "\n",
    "        if \"dataset\" not in df:\n",
    "            df[\"dataset\"] = dataset_name\n",
    "        if \"ligand_sequence\" not in df:\n",
    "            df[\"ligand_sequence\"] = None\n",
    "\n",
    "        # Add a unique id\n",
    "        df[\"unique_id\"] = [\n",
    "            get_unique_id(dataset, effect_type, protein_sequence, ligand_sequence)\n",
    "            for dataset, effect_type, protein_sequence, ligand_sequence in df[\n",
    "                [\"dataset\", \"effect_type\", \"protein_sequence\", \"ligand_sequence\"]\n",
    "            ].values\n",
    "        ]\n",
    "        unique_ids = set(df[\"unique_id\"].values)\n",
    "        assert len(unique_ids) == len(df)\n",
    "        assert not set(unique_ids) & _seen[coi]\n",
    "        _seen[coi].update(unique_ids)\n",
    "\n",
    "        # Add extra columns\n",
    "        if columns_all is None:\n",
    "            columns_all = columns + [c for c in extra_columns if c in df]\n",
    "\n",
    "        df_out = df[columns_all]\n",
    "\n",
    "        # Write files\n",
    "        row_per_batch = 100\n",
    "        for start in range(0, len(df_out), row_per_batch):\n",
    "            stop = start + row_per_batch\n",
    "            df_out_slice = df_out.iloc[start:stop]\n",
    "            table = pa.Table.from_pandas(df_out_slice, preserve_index=False)\n",
    "\n",
    "            if writer is None:\n",
    "                output_file = output_dir.joinpath(f\"{dataset_name}.parquet\")\n",
    "                writer = pq.ParquetWriter(output_file, table.schema)\n",
    "                print(output_file)\n",
    "\n",
    "            writer.write_table(table)\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfile = pq.ParquetFile(\"/home/kimlab1/database_data/datapkg_output_dir/elaspic-v2/01_load_data_core/mutation-ssm2.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_seen.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfile.num_row_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pfile.read_row_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pa.Table.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "30_000 / 300"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
