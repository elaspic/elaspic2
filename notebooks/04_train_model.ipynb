{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shlex\n",
    "import subprocess\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "import optuna\n",
    "import concurrent.futures\n",
    "import itertools\n",
    "import lightgbm\n",
    "import json\n",
    "import lightgbm as lgb\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import math\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import torch\n",
    "from scipy import stats\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "from tqdm.notebook import tqdm\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"max_columns\", 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paramters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTEBOOK_DIR = Path(\"04_train_model\").resolve()\n",
    "NOTEBOOK_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "NOTEBOOK_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COI = \"interface\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_VERSION = \"v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"DATAPKG_OUTPUT_DIR\" in os.environ:\n",
    "    OUTPUT_DIR = Path(os.getenv(\"DATAPKG_OUTPUT_DIR\")).joinpath(\"elaspic2\").resolve()\n",
    "else:\n",
    "    OUTPUT_DIR = NOTEBOOK_DIR.parent\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (slurm_tmpdir := os.getenv(\"SLURM_TMPDIR\")) is not None:\n",
    "    os.environ[\"TMPDIR\"] = slurm_tmpdir\n",
    "\n",
    "print(tempfile.gettempdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if COI == \"core\":\n",
    "    datasets = [\n",
    "        \"elaspic-training-set-core\",\n",
    "        \"protherm-dagger-core\",\n",
    "        \"rocklin-2017-core\",\n",
    "        \"dunham-2020-core\",\n",
    "        \"starr-2020-core\",\n",
    "        \"cagi5-frataxin-core\",\n",
    "        \"huang-2020-core\",\n",
    "    ]\n",
    "else:\n",
    "    assert COI == \"interface\"\n",
    "    datasets = [\n",
    "        \"elaspic-training-set-interface\",\n",
    "        \"skempi-v2-interface\",\n",
    "#         \"intact-mutations-interface\",\n",
    "        \"dunham-2020-interface\",\n",
    "        \"starr-2020-interface\",\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_generators = [\n",
    "    \"02_run_rosetta_ddg\",\n",
    "    \"02_run_proteinsolver\",\n",
    "    \"02_run_protbert\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_mutations(df):\n",
    "    results = []\n",
    "    for row in df.itertuples():\n",
    "        for idx in range(len(row.mutation)):\n",
    "            row_mut = {\n",
    "                \"unique_id\": row.unique_id,\n",
    "                \"dataset\": row.dataset,\n",
    "                \"name\": row.name,\n",
    "                \"mutation\": row.mutation[idx],\n",
    "                \"effect\": row.effect[idx],\n",
    "                \"effect_type\": row.effect_type,\n",
    "            }\n",
    "            for column in [\"provean_score\", \"foldx_score\", \"elaspic_score\"]:\n",
    "                if hasattr(row, column):\n",
    "                    row_mut[column] = getattr(row, column)[idx]\n",
    "            results.append(row_mut)\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_mutation_complement(df):\n",
    "    df = df.copy()\n",
    "    df[\"rev\"] = False\n",
    "\n",
    "    df_comp = df.copy()\n",
    "    df_comp[\"rev\"] = True\n",
    "    df_comp[\"mutation\"] = (\n",
    "        df_comp[\"mutation\"].str[-1] + df_comp[\"mutation\"].str[1:-1] + df_comp[\"mutation\"].str[0]\n",
    "    )\n",
    "    for column in [\"effect\", \"provean_score\", \"foldx_score\", \"elaspic_score\"]:\n",
    "        if column in df_comp:\n",
    "            df_comp[column] = -df_comp[column]\n",
    "    for column in df_comp:\n",
    "        if column.endswith(\"_wt\"):\n",
    "            column_mut = column[:-3] + \"_mut\"\n",
    "            df_comp[column], df_comp[column_mut] = (\n",
    "                df_comp[column_mut].copy(),\n",
    "                df_comp[column].copy(),\n",
    "            )\n",
    "\n",
    "    df_out = pd.concat([df, df_comp], ignore_index=True)\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df = pd.DataFrame(\n",
    "    [[0, \"M1A\", 1.234, \"wt score\", \"mut score\"], [1, \"M2C\", -0.05, \"wt score 2\", \"mut score 2\"]],\n",
    "    columns=[\"unique_id\", \"mutation\", \"effect\", \"feature_wt\", \"feature_mut\"],\n",
    ")\n",
    "\n",
    "tmp2_df = add_mutation_complement(tmp_df)\n",
    "\n",
    "display(tmp_df)\n",
    "display(tmp2_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_feature_dfs(feature_dfs):\n",
    "    def _clean_df(df):\n",
    "        df = df.copy()\n",
    "        assert len(df) == len(df[[\"unique_id\", \"mutation\"]].drop_duplicates())\n",
    "        for column in [\"effect\", \"effect_type\", \"provean_score\", \"foldx_score\", \"elaspic_score\"]:\n",
    "            if column in df:\n",
    "                del df[column]\n",
    "        return df\n",
    "\n",
    "    if not feature_dfs:\n",
    "        return None\n",
    "\n",
    "    df = _clean_df(feature_dfs[0])\n",
    "    for other_df in feature_dfs[1:]:\n",
    "        df = df.merge(\n",
    "            _clean_df(other_df), how=\"outer\", on=[\"unique_id\", \"mutation\", \"rev\"]\n",
    "        )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = {}\n",
    "for dataset_name in datasets:\n",
    "    input_file = OUTPUT_DIR.joinpath(\"01_load_data\", f\"{dataset_name}.parquet\")\n",
    "    pfile = pq.ParquetFile(input_file)\n",
    "    task_count = pfile.num_row_groups\n",
    "    df = pfile.read().to_pandas(integer_object_nulls=True)\n",
    "    expanded_df = (\n",
    "        add_mutation_complement(expand_mutations(df))\n",
    "#         expand_mutations(df)\n",
    "        .drop_duplicates(subset=[\"unique_id\", \"mutation\"])\n",
    "        .sort_values([\"unique_id\", \"mutation\"])\n",
    "    )\n",
    "#     expanded_df[\"rev\"] = False\n",
    "    sequence_df = df[[\"unique_id\", \"protein_sequence\", \"ligand_sequence\"]].drop_duplicates()\n",
    "\n",
    "    keys = set(tuple(x) for x in expanded_df[[\"unique_id\", \"mutation\", \"rev\"]].values)\n",
    "    \n",
    "    features = {}\n",
    "    for feature_generator in feature_generators:\n",
    "        output_dir = OUTPUT_DIR.joinpath(feature_generator)\n",
    "        feature_dfs = []\n",
    "        for task_id in range(1, task_count + 1):\n",
    "            output_file_template = \"{dataset_name}-{task_prefix}{task_id}{task_suffix}-{task_count}.parquet\"\n",
    "\n",
    "            if feature_generator in [\"02_run_rosetta_ddg\"]:\n",
    "                task_prefix_rev_list = [(\"wt2mut-\", False), (\"mut2wt-\", True)]\n",
    "            else:\n",
    "                task_prefix_rev_list = [(\"\", None)]\n",
    "\n",
    "            for (task_prefix, rev) in task_prefix_rev_list:\n",
    "                output_file_kwargs = dict(\n",
    "                    dataset_name=dataset_name,\n",
    "                    task_prefix=task_prefix,\n",
    "                    task_id=task_id,\n",
    "                    task_count=task_count,\n",
    "                )\n",
    "                output_file = OUTPUT_DIR.joinpath(\n",
    "                    feature_generator,\n",
    "                    output_file_template.format(task_suffix=\"\", **output_file_kwargs)\n",
    "                ).resolve()\n",
    "                if output_file.is_file():\n",
    "                    feature_df = pq.read_table(output_file).to_pandas(integer_object_nulls=True)\n",
    "                else:\n",
    "                    subtask_feature_dfs = []\n",
    "                    subtask_missing_files = []\n",
    "                    for subtask_idx in range(20):\n",
    "                        subtask_output_file = OUTPUT_DIR.joinpath(\n",
    "                            feature_generator,\n",
    "                            output_file_template.format(task_suffix=string.ascii_lowercase[subtask_idx], **output_file_kwargs)\n",
    "                        ).resolve()\n",
    "                        if subtask_output_file.is_file():\n",
    "                            feature_df = pq.read_table(subtask_output_file).to_pandas(integer_object_nulls=True)\n",
    "                            subtask_feature_dfs.append(feature_df)\n",
    "                        else:\n",
    "                            subtask_missing_files.append(subtask_output_file)\n",
    "                    if subtask_feature_dfs:\n",
    "                        feature_df = pd.concat(subtask_feature_dfs, ignore_index=True)\n",
    "                        if subtask_missing_files:\n",
    "                            for subtask_missing_file in subtask_missing_files:\n",
    "                                print(f\"File {subtask_missing_file} is missing. Skipping...\")\n",
    "                    else:\n",
    "                        print(f\"File {output_file} is missing. Skipping...\")\n",
    "                        continue\n",
    "\n",
    "                if feature_df.empty:\n",
    "                    print(f\"File {output_file} contains no data. Skipping...\")\n",
    "                    continue\n",
    "                \n",
    "                if rev in [True, False]:\n",
    "                    feature_df[\"rev\"] = rev\n",
    "                else:\n",
    "                    feature_df = add_mutation_complement(feature_df)\n",
    "                if rev is True:\n",
    "                    feature_df[\"unique_id\"] = -feature_df[\"unique_id\"].values\n",
    "                assert not set(tuple(x) for x in feature_df[[\"unique_id\", \"mutation\", \"rev\"]].values) - keys, (dataset_name, feature_generator, task_id)    \n",
    "                feature_dfs.append(feature_df)\n",
    "\n",
    "        if not feature_dfs:\n",
    "            print(\n",
    "                f\"No data collected for dataset {dataset_name} and feature generator {feature_generator}.\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        final_feature_df = pd.concat(feature_dfs, ignore_index=True)\n",
    "        features[feature_generator] = final_feature_df\n",
    "\n",
    "    input_data[dataset_name] = {\n",
    "        \"expanded_df\": expanded_df,\n",
    "        \"sequence_df\": sequence_df,\n",
    "        \"feature_df\": merge_feature_dfs(list(features.values())),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_df = pd.concat(\n",
    "    [d[\"expanded_df\"] for d in input_data.values() if d[\"feature_df\"] is not None]\n",
    ")\n",
    "\n",
    "sequence_df = pd.concat(\n",
    "    [d[\"sequence_df\"] for d in input_data.values() if d[\"feature_df\"] is not None]\n",
    ")\n",
    "\n",
    "features_df = pd.concat(\n",
    "    [d[\"feature_df\"] for d in input_data.values() if d[\"feature_df\"] is not None]\n",
    ").sort_values([\"unique_id\", \"mutation\"])\n",
    "assert features_df[\"unique_id\"].min() >= 0\n",
    "len(features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_wn_df = expanded_df.merge(features_df, on=[\"unique_id\", \"mutation\", \"rev\"], validate=\"1:1\", how=\"outer\")\n",
    "# assert len(input_wn_df) == len(features_df), (len(expanded_df), len(features_df), len(input_wn_df))\n",
    "assert input_wn_df[\"dataset\"].notnull().all()\n",
    "print(\n",
    "    f\"Lost {len(expanded_df) - len(features_df):,} out of {len(expanded_df):,} rows due to missing features.\"\n",
    ")\n",
    "\n",
    "# Correct the sign on some features\n",
    "for dataset, effect_type in [\n",
    "    (\"protherm-dagger-core\", \"-ΔΔG\"),\n",
    "    (\"rocklin-2017-core\", \"Stability score change\"),\n",
    "    (\"dunham_2020_tianyu\", \"Deep mutation scan\"),\n",
    "    (\"starr_2020_tianyu\", \"Deep mutation scan\"),\n",
    "]:\n",
    "    mask = (input_wn_df[\"dataset\"] == dataset) & (input_wn_df[\"effect_type\"] == effect_type)\n",
    "    if mask.any():\n",
    "        print(f\"Reversing sign for {dataset} ({effect_type})...\")\n",
    "        input_wn_df.loc[mask, \"effect\"] = -input_wn_df.loc[mask, \"effect\"]\n",
    "        if effect_type == \"-ΔΔG\":\n",
    "            input_wn_df.loc[mask, \"effect_type\"] = \"ΔΔG\"\n",
    "\n",
    "len(input_wn_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [c for c in input_wn_df if c.startswith(\"protbert_\")]\n",
    "input_wn_df[columns].isnull().sum()  # 194"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [c for c in input_wn_df if c.startswith(\"proteinsolver_\")]\n",
    "input_wn_df[columns].isnull().sum()  # 308"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [c for c in input_wn_df if c.startswith(\"rosetta_\")]\n",
    "input_wn_df[columns].isnull().sum().head()  # 79,025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove rows with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df = input_wn_df.dropna(\n",
    "    subset=[\n",
    "        c for c in input_wn_df if c.startswith(\"protbert_\") or c.startswith(\"proteinsolver_\")\n",
    "    ]\n",
    ")\n",
    "print(\n",
    "    f\"Lost {len(input_wn_df) - len(input_df):,} out of {len(input_wn_df):,} rows due to missing features.\"\n",
    ")\n",
    "\n",
    "_before = len(input_df)\n",
    "input_df = input_df[~input_df[\"effect\"].isnull()]\n",
    "print(\n",
    "    f\"Lost {_before - len(input_df):,} out of {_before:,} rows due to missing effect values.\"\n",
    ")\n",
    "\n",
    "input_df = input_df.copy()\n",
    "\n",
    "len(input_df)  # Core: 642160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not input_df[\"effect\"].isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_delta(input_df, column, column_ref, column_change):\n",
    "    pca_columns = []\n",
    "    value_sample = input_df[column].iloc[0]\n",
    "    if isinstance(value_sample, (list, np.ndarray)):\n",
    "        input_df[column_change] = input_df[column] - input_df[column_ref]\n",
    "        return True\n",
    "    else:\n",
    "        input_df[column_change] = input_df[column] - input_df[column_ref]\n",
    "        return False\n",
    "\n",
    "\n",
    "pca_columns = []\n",
    "for column in sorted(input_df):\n",
    "    if column.endswith(\"_mut\") and \"_core2interface_\" not in column:\n",
    "        print(column, \"(wt → mut)\")\n",
    "        column_ref = column[:-4] + \"_wt\"\n",
    "        column_change = column[:-4] + \"_change\"\n",
    "        if assign_delta(input_df, column, column_ref, column_change):\n",
    "            pca_columns.extend([column_ref, column_change])\n",
    "\n",
    "for column in sorted(input_df):\n",
    "    if \"_interface_\" in column and not column.endswith(\"_mut\"):\n",
    "        print(column, \"(core → interface)\")\n",
    "        column_ref = column.replace(\"_interface_\", \"_core_\")\n",
    "        column_change = column.replace(\"_interface_\", \"_core2interface_\")\n",
    "        if assign_delta(input_df, column, column_ref, column_change):\n",
    "            pca_columns.extend([column_change])\n",
    "\n",
    "pca_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove invalid datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df[\"dataset\"].value_counts()\n",
    "# CORE\n",
    "# cosmic                  469802\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if COI == \"core\":\n",
    "    datasets_to_drop = {\n",
    "        \"cagi4_sumo_ligase\",\n",
    "        \"benedix_et_al\",\n",
    "        \"hiv_escape_mutations\",\n",
    "        \"ab_bind\",\n",
    "        \"skempiskempi\",\n",
    "        \"taipale_ppi\",\n",
    "        # \"cosmic\",\n",
    "    }\n",
    "else:\n",
    "    datasets_to_drop = {\n",
    "        \"cagi4_sumo_ligase\",\n",
    "        \"benedix_et_al\",\n",
    "        \"hiv_escape_mutations\",\n",
    "        \"taipale\",\n",
    "    }\n",
    "\n",
    "input_df = input_df[~input_df[\"dataset\"].isin(datasets_to_drop)]\n",
    "\n",
    "input_df[\"dataset\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (dataset, effect_type), gp in input_df.groupby([\"dataset\", \"effect_type\"]):\n",
    "    gp = gp.copy()\n",
    "    gp_sub = gp.dropna(subset=[\"effect\", \"protbert_core_score_change\"])\n",
    "    corr1 = stats.spearmanr(gp_sub[\"effect\"], gp_sub[\"protbert_core_score_change\"])\n",
    "    gp_sub = gp_sub[gp_sub[\"rev\"] == False]\n",
    "    corr2 = stats.spearmanr(gp_sub[\"effect\"], gp_sub[\"protbert_core_score_change\"])\n",
    "    if corr1[0] > 0 or corr2[0] > 0:\n",
    "        print(dataset, effect_type)\n",
    "        for column in [\n",
    "            \"provean_score\",\n",
    "            \"foldx_score\",\n",
    "            \"elaspic_score\",\n",
    "            \"protbert_core_score_change\",\n",
    "            \"proteinsolver_core_score_change\",\n",
    "        ]:\n",
    "            gp_sub = gp.dropna(subset=[\"effect\", column])\n",
    "            corr = stats.spearmanr(gp_sub[\"effect\"], gp_sub[column])\n",
    "            print(f\"{column:30s} {corr[0]:+.4} {corr[1]:.4}\")\n",
    "            gp_sub = gp_sub[gp_sub[\"rev\"] == False]\n",
    "            corr = stats.spearmanr(gp_sub[\"effect\"], gp_sub[column])\n",
    "            print(f\"{column:30s} {corr[0]:+.4} {corr[1]:.4}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (dataset, effect_type), gp in input_df.groupby([\"dataset\", \"effect_type\"]):\n",
    "    gp = gp.dropna(subset=[\"effect\", \"protbert_core_score_change\"])\n",
    "    assert len(gp)\n",
    "    corr = stats.spearmanr(gp[\"effect\"], gp[\"protbert_core_score_change\"])\n",
    "    assert corr[0] <= 0, (dataset, effect_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "humsavar_unique_ids = set(input_df[input_df[\"dataset\"] == \"humsavar\"][\"unique_id\"].unique())\n",
    "humsavar_sequences = set(tuple(s) for s in sequence_df[sequence_df[\"unique_id\"].isin(humsavar_unique_ids)][[\"protein_sequence\", \"ligand_sequence\"]].values)\n",
    "len(input_df)  # 638184"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinvar_unique_ids = set(input_df[input_df[\"dataset\"] == \"clinvar\"][\"unique_id\"].unique())\n",
    "_before = len(clinvar_unique_ids)\n",
    "clinvar_unique_ids = {\n",
    "    uid for uid, pseq, lseq\n",
    "    in sequence_df[sequence_df[\"unique_id\"].isin(clinvar_unique_ids)][[\"unique_id\", \"protein_sequence\", \"ligand_sequence\"]].values\n",
    "    if (pseq, lseq) not in humsavar_sequences\n",
    "}\n",
    "print(f\"Removed {_before - len(clinvar_unique_ids)} clinvar unique ids.\")\n",
    "\n",
    "input_df = input_df[(input_df[\"dataset\"] != \"clinvar\") | (input_df[\"unique_id\"].isin(clinvar_unique_ids))]\n",
    "len(input_df)  # 617500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinvar_sequences = set(tuple(s) for s in sequence_df[sequence_df[\"unique_id\"].isin(clinvar_unique_ids)][[\"protein_sequence\", \"ligand_sequence\"]].values)\n",
    "\n",
    "cosmic_unique_ids = set(input_df[input_df[\"dataset\"] == \"cosmic\"][\"unique_id\"].unique())\n",
    "_before = len(cosmic_unique_ids)\n",
    "cosmic_unique_ids = {\n",
    "    uid for uid, pseq, lseq\n",
    "    in sequence_df[sequence_df[\"unique_id\"].isin(cosmic_unique_ids)][[\"unique_id\", \"protein_sequence\", \"ligand_sequence\"]].values\n",
    "    if (pseq, lseq) not in humsavar_sequences and (pseq, lseq) not in clinvar_sequences\n",
    "}\n",
    "print(f\"Removed {_before - len(cosmic_unique_ids)} cosmic unique ids.\")\n",
    "\n",
    "input_df = input_df[(input_df[\"dataset\"] != \"cosmic\") | (input_df[\"unique_id\"].isin(cosmic_unique_ids))]\n",
    "len(input_df)  # 516344"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df[\"dataset\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster by sequence identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_clusters(input_sequences, min_seq_id=0.3):\n",
    "    with tempfile.TemporaryDirectory() as tmp_dir:\n",
    "        input_dir = Path(tmp_dir, \"input\")\n",
    "        input_dir.mkdir()\n",
    "\n",
    "        output_dir = Path(tmp_dir, \"output\")\n",
    "        output_dir.mkdir()\n",
    "\n",
    "        scratch_dir = Path(tmp_dir, \"scratch\")\n",
    "        scratch_dir.mkdir()\n",
    "\n",
    "        with input_dir.joinpath(\"input.fasta\").open(\"wt\") as fout:\n",
    "            for tup in input_sequences.itertuples():\n",
    "                fout.write(f\">{tup.unique_id}\\n{tup.protein_sequence}\\n\")\n",
    "\n",
    "        system_command = f\"mmseqs easy-cluster --min-seq-id {min_seq_id} '{input_dir}/input.fasta' '{output_dir}/result' '{scratch_dir}'\"\n",
    "        print(system_command)\n",
    "\n",
    "        proc = subprocess.run(shlex.split(system_command), capture_output=True, check=True)\n",
    "\n",
    "        cluster_df = pd.read_csv(\n",
    "            output_dir.joinpath(\"result_cluster.tsv\"), sep=\"\\t\", names=[\"cluster_id\", \"unique_id\"]\n",
    "        )\n",
    "        assert len(cluster_df) == len(cluster_df[\"unique_id\"].unique())\n",
    "\n",
    "    return cluster_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequences = sequence_df.merge(input_df[[\"unique_id\"]].drop_duplicates())\n",
    "\n",
    "len(input_sequences)  # CORE: 13779"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_df = obtain_clusters(input_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"cluster_id\" in input_df:\n",
    "    del input_df[\"cluster_id\"]\n",
    "\n",
    "input_df = input_df.merge(cluster_df, on=\"unique_id\", how=\"outer\", validate=\"m:1\")\n",
    "assert input_df[\"cluster_id\"].notnull().all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract out independent test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if COI == \"core\":\n",
    "    test_datasets = {\n",
    "        \"starr_2020_tianyu\",\n",
    "        \"huang_2020\",\n",
    "        \"cagi5_frataxin\",\n",
    "    }\n",
    "else:\n",
    "    test_datasets = {\n",
    "        \"starr_2020_tianyu\",\n",
    "    }\n",
    "input_test_df = input_df[input_df[\"dataset\"].isin(test_datasets)].copy()\n",
    "\n",
    "print(input_test_df[\"dataset\"].unique())\n",
    "print(len(input_test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_cluster_ids = set(input_test_df[\"cluster_id\"])  # TODO: \n",
    "\n",
    "input_train_df = input_df[~input_df[\"dataset\"].isin(test_datasets)].copy()\n",
    "\n",
    "print(input_train_df[\"dataset\"].unique())\n",
    "print(len(input_train_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train / validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "def _update_mapping(df, mapping, num_folds):\n",
    "    @dataclass(order=True)\n",
    "    class PrioritizedItem:\n",
    "        priority: int\n",
    "        idx: int = field(compare=False)\n",
    "        data: Any = field(compare=False)\n",
    "\n",
    "    pq = [PrioritizedItem(0, i, []) for i in range(num_folds)]\n",
    "    for cluster_id, gp in df.groupby(\"cluster_id\"):\n",
    "        if cluster_id in mapping:\n",
    "            item_idx = mapping[cluster_id]\n",
    "            item = next(item for item in pq if item.idx == item_idx)\n",
    "            item.priority += len(gp)\n",
    "            item.data.append(cluster_id)\n",
    "            heapq.heapify(pq)\n",
    "        else:\n",
    "            item = heapq.heappop(pq)\n",
    "            item.priority += len(gp)\n",
    "            item.data.append(cluster_id)\n",
    "            heapq.heappush(pq, item)\n",
    "\n",
    "    for item in pq:\n",
    "        for cluster_id in item.data:\n",
    "            if cluster_id in mapping:\n",
    "                assert mapping[cluster_id] == item.idx\n",
    "            else:\n",
    "                mapping[cluster_id] = item.idx\n",
    "    \n",
    "    return mapping\n",
    "\n",
    "\n",
    "def map_to_test_fold(input_df, effect_types, num_folds):\n",
    "    dfs = [input_df[input_df[\"effect_type\"] == effect_type] for effect_type in effect_types]\n",
    "    assert sum(len(df) for df in dfs) == len(input_df)\n",
    "\n",
    "    mapping = {}\n",
    "    for df in dfs:\n",
    "        mapping = _update_mapping(df, mapping, num_folds)\n",
    "\n",
    "    return mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_train_df[\"effect_type\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if COI == \"core\":\n",
    "    num_folds = 6\n",
    "else:\n",
    "    num_folds = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_id_to_test_fold_mapping = map_to_test_fold(\n",
    "    input_train_df,\n",
    "    [\"ΔΔG\", \"ΔΔG (from Kon/Koff)\", \"ΔΔG (from affinity)\",\n",
    "     \"Stability score change\",\n",
    "     \"Deep mutation scan\",\n",
    "     \"Deleteriousness score\", \"Deleteriousness class\"], num_folds=num_folds)\n",
    "input_train_df[\"test_fold\"] = input_train_df[\"cluster_id\"].map(cluster_id_to_test_fold_mapping)\n",
    "assert input_train_df[\"test_fold\"].notnull().all()\n",
    "assert len(input_train_df[\"test_fold\"].unique()) == num_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_train_df[\"test_fold\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train PCA models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 10\n",
    "for column in pca_columns:\n",
    "    print(column)\n",
    "    values = np.vstack(input_train_df[column].values)\n",
    "\n",
    "    pickle_file = NOTEBOOK_DIR.joinpath(f\"pca-{column}-{COI}.pickle\")\n",
    "    if pickle_file.is_file():\n",
    "        pca = torch.load(pickle_file)\n",
    "    else:\n",
    "        pca = PCA(n_components=n_components)\n",
    "        pca.fit(values)\n",
    "        torch.save(pca, pickle_file)\n",
    "\n",
    "    values_out = pca.transform(values)\n",
    "    for i in range(n_components):\n",
    "        new_column = f\"{column}_{i}_pc\"\n",
    "        input_train_df[new_column] = values_out[:, i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_splits = []\n",
    "ps = PredefinedSplit(input_train_df[\"test_fold\"])\n",
    "for split_idx, (train, test) in enumerate(tqdm(ps.split(), total=n_components)):\n",
    "    train_df = input_train_df.iloc[train].sample(frac=1.0, replace=False).sort_values([\"unique_id\"]).copy()\n",
    "    test_df = input_train_df.iloc[test].sample(frac=1.0, replace=False).sort_values([\"unique_id\"]).copy()\n",
    "    assert not set(train_df[\"cluster_id\"]) & set(test_df[\"cluster_id\"])\n",
    "\n",
    "    first_row = train_df.iloc[0]\n",
    "    for column in list(train_df):\n",
    "        value = first_row[column]\n",
    "        if isinstance(value, (list, tuple, np.ndarray)):\n",
    "            del train_df[column], test_df[column]\n",
    "\n",
    "    train_test_splits.append((train_df, test_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTEBOOK_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with NOTEBOOK_DIR.parent.joinpath(\"04_train_model\", f\"pca-columns-{COI}.{DATASET_VERSION}.parquet\").open(\"wt\") as fout:\n",
    "    json.dump(pca_columns, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = NOTEBOOK_DIR.parent.joinpath(\"04_train_model\", f\"sequences-{COI}.{DATASET_VERSION}.parquet\")\n",
    "\n",
    "pq.write_table(\n",
    "    pa.Table.from_pandas(sequence_df, preserve_index=False),\n",
    "    output_file,\n",
    "    row_group_size=1_000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = NOTEBOOK_DIR.parent.joinpath(\"04_train_model\", f\"input-train-{COI}.{DATASET_VERSION}.parquet\")\n",
    "\n",
    "pq.write_table(\n",
    "    pa.Table.from_pandas(input_train_df, preserve_index=False),\n",
    "    output_file,\n",
    "    row_group_size=10_000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = NOTEBOOK_DIR.parent.joinpath(\"04_train_model\", f\"input-test-{COI}.{DATASET_VERSION}.parquet\")\n",
    "\n",
    "pq.write_table(\n",
    "    pa.Table.from_pandas(input_test_df, preserve_index=False),\n",
    "    output_file,\n",
    "    row_group_size=10_000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx, (train_df, test_df) in enumerate(train_test_splits):\n",
    "#     print(idx)\n",
    "\n",
    "#     output_file = NOTEBOOK_DIR.parent.joinpath(\"04_train_model\", f\"xval-train-{COI}-{idx}.{DATASET_VERSION}.parquet\")\n",
    "#     pq.write_table(\n",
    "#         pa.Table.from_pandas(train_df, preserve_index=False),\n",
    "#         output_file,\n",
    "#         row_group_size=10_000,\n",
    "#     )\n",
    "    \n",
    "#     output_file = NOTEBOOK_DIR.parent.joinpath(\"04_train_model\", f\"xval-test-{COI}-{idx}.{DATASET_VERSION}.parquet\")\n",
    "#     pq.write_table(\n",
    "#         pa.Table.from_pandas(test_df, preserve_index=False),\n",
    "#         output_file,\n",
    "#         row_group_size=10_000,\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [\n",
    "    c\n",
    "    for c in list(train_test_splits[0][0])\n",
    "    if (c.endswith(\"_wt\") or c.endswith(\"_mut\") or c.endswith(\"_change\") or c.endswith(\"_pc\"))\n",
    "    and not (c.endswith(\"dg_change\") or c.startswith(\"rosetta_\"))\n",
    "]\n",
    "\n",
    "# feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_columns = [c for c in list(train_test_splits[0][0]) if c not in feature_columns]\n",
    "\n",
    "# other_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(df):\n",
    "    effect = df[\"effect\"].values.copy()\n",
    "\n",
    "    mask = df[\"effect_type\"].str.startswith(\"ΔΔG\")\n",
    "    effect[mask] *= 0.8\n",
    "\n",
    "    mask = df[\"effect_type\"] == \"Deleteriousness class\"\n",
    "    effect[mask] *= 1\n",
    "\n",
    "    mask = df[\"effect_type\"] == \"Stability score change\"\n",
    "    effect[mask] *= 5\n",
    "\n",
    "    mask = df[\"effect_type\"] == \"Deleteriousness score\"\n",
    "    if mask.any():\n",
    "        assert effect[mask].min() >= -5 and effect[mask].max() <= 5\n",
    "\n",
    "    mask = df[\"effect_type\"] == \"Deep mutation scan\"\n",
    "    effect[mask] *= 4\n",
    "\n",
    "    effect = np.rint(np.clip(effect, -5, 5) * 100 + 500)\n",
    "    return effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_train_df[\"effect_type\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist(get_label(input_train_df), bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist(get_label(input_train_df[input_train_df[\"effect_type\"] == 'Deleteriousness score']), bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist(get_label(input_train_df[input_train_df[\"effect_type\"] == 'Stability score change']), bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist(get_label(input_train_df[input_train_df[\"effect_type\"] == 'Deep mutation scan']), bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist(get_label(input_train_df[input_train_df[\"effect_type\"] == 'ΔΔG']), bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist(get_label(input_train_df[input_train_df[\"effect_type\"] == 'ΔΔG (from affinity)']), bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist(get_label(input_train_df[input_train_df[\"effect_type\"] == 'ΔΔG (from Kon/Koff)']), bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assert_get_group_valid(df):\n",
    "    assert df[\"unique_id\"].is_monotonic_increasing\n",
    "    \n",
    "    prev = None\n",
    "    for unique_id, rev in df[['unique_id', \"rev\"]].values:\n",
    "        if prev is not None:\n",
    "            if not rev:\n",
    "                assert unique_id != prev[0] or not prev[1], (unique_id, rev, prev)\n",
    "            else:\n",
    "                assert unique_id == prev[0]\n",
    "        prev = (unique_id, rev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_group(df, max_group_size=100):\n",
    "    assert df[\"unique_id\"].is_monotonic_increasing\n",
    "    vc = df[\"unique_id\"].value_counts()\n",
    "    groups = [vc[uid] for uid in df[\"unique_id\"].unique()]\n",
    "    if max_group_size:\n",
    "        old_groups, groups = groups, []\n",
    "        for idx, group in enumerate(old_groups):\n",
    "            if group <= max_group_size:\n",
    "                groups.append(group)\n",
    "            else:\n",
    "                num_subgroups = math.ceil(group / max_group_size)\n",
    "                num_per_group = math.floor(group / num_subgroups)\n",
    "                subgroups = [num_per_group] * num_subgroups\n",
    "                if (remainder := group - sum(subgroups)):\n",
    "                    assert remainder < num_subgroups\n",
    "                    for remainder_idx in range(remainder):\n",
    "                        subgroups[remainder_idx] += 1\n",
    "                groups.extend(subgroups)\n",
    "    assert sum(groups) == len(df), (sum(groups), len(df))\n",
    "    assert not max_group_size or max(groups) <= max_group_size\n",
    "    return np.array(groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if COI == \"core\":\n",
    "    max_group_size = 100\n",
    "else:\n",
    "    max_group_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.clip(get_group(input_train_df.sort_values([\"unique_id\"]), max_group_size), 0, max_group_size), bins=100)\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(input, param, early_stopping_rounds=10):\n",
    "    train_df, test_df = input\n",
    "\n",
    "    train_ds = lgb.Dataset(\n",
    "        train_df[feature_columns],\n",
    "        label=get_label(train_df),\n",
    "        group=get_group(train_df, max_group_size=max_group_size),\n",
    "    )\n",
    "\n",
    "    valid_ds = lgb.Dataset(\n",
    "        test_df[feature_columns],\n",
    "        label=get_label(test_df),\n",
    "        group=get_group(test_df, max_group_size=max_group_size),\n",
    "        reference=train_ds,\n",
    "    )\n",
    "\n",
    "    bst = lgb.train(\n",
    "        param,\n",
    "        train_ds,\n",
    "        valid_sets=[valid_ds],\n",
    "        num_boost_round=100,\n",
    "        verbose_eval=False,\n",
    "        # feval=my_feval,\n",
    "        # early_stopping_rounds=early_stopping_rounds,\n",
    "    )\n",
    "\n",
    "    return bst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skempi_unique_ids = set(input_train_df[input_train_df[\"dataset\"] == \"skempi++\"][\"unique_id\"].unique())\n",
    "skempi_sequences = set(tuple(s) for s in sequence_df[sequence_df[\"unique_id\"].isin(skempi_unique_ids)][[\"protein_sequence\", \"ligand_sequence\"]].values)\n",
    "\n",
    "skempi_v2_unique_ids = set(input_train_df[input_train_df[\"dataset\"] == \"skempi-v2\"][\"unique_id\"].unique())\n",
    "skempi_v2_unique_ids = {\n",
    "    uid for uid, pseq, lseq\n",
    "    in sequence_df[sequence_df[\"unique_id\"].isin(skempi_v2_unique_ids)][[\"unique_id\", \"protein_sequence\", \"ligand_sequence\"]].values\n",
    "    if (pseq, lseq) not in skempi_sequences\n",
    "}\n",
    "\n",
    "\n",
    "def get_aggregate_spearmanr(result_df, datasets):\n",
    "    corrs = []\n",
    "    for dataset, effect_type, *_ in datasets:\n",
    "        df = result_df[\n",
    "            (result_df[\"dataset\"] == dataset)\n",
    "            & (result_df[\"effect_type\"] == effect_type)\n",
    "            & (result_df[\"rev\"] == False)\n",
    "        ]\n",
    "\n",
    "        if dataset == \"skempi-v2\":\n",
    "            df = df[df[\"unique_id\"].isin(skempi_v2_unique_ids)]\n",
    "\n",
    "        df = df.dropna(subset=[\"effect\", \"ddg_pred\"])\n",
    "        \n",
    "        corr = stats.spearmanr(df[\"effect\"], df[\"ddg_pred\"])[0]\n",
    "        corrs.append(corr)\n",
    "    return sum(corrs) / len(corrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if COI == \"core\":\n",
    "    columns_full = [\n",
    "        \"ddg_pred\",\n",
    "        \"elaspic_score\",\n",
    "        \"foldx_score\",\n",
    "        \"rosetta_dg_change\",\n",
    "    ]\n",
    "\n",
    "    datasets_eval = [\n",
    "        [\"protherm++\", \"ΔΔG\", columns_full],\n",
    "        [\"humsavar\", \"Deleteriousness class\", columns_full],\n",
    "        [\"clinvar\", \"Deleteriousness class\", columns_full],\n",
    "        [\"cosmic\", \"Deleteriousness class\", columns_full],\n",
    "        [\"taipale\", \"ΔΔG\", columns_full],\n",
    "        # [\"taipale_gpca\", \"ΔΔG\", columns_full],\n",
    "        # [\"cagi5_frataxin\", \"ΔΔG\", [\"ddg_pred\"]],\n",
    "        [\"rocklin-2017-core\", \"Stability score change\", [\"ddg_pred\", \"rosetta_dg_change\"]],\n",
    "        [\"dunham_2020_tianyu\", \"Deep mutation scan\", [\"ddg_pred\", \"rosetta_dg_change\"]],\n",
    "        # [\"protherm-dagger-core\", \"ΔΔG\", [\"ddg_pred\", \"rosetta_dg_change\"]],\n",
    "    ]\n",
    "else:\n",
    "    columns_full = [\n",
    "        \"ddg_pred\",\n",
    "        \"elaspic_score\",\n",
    "        \"foldx_score\",\n",
    "        \"rosetta_complex_dg_change\",\n",
    "    ]\n",
    "\n",
    "    datasets_eval = [\n",
    "        [\"skempi++\", \"ΔΔG\", columns_full],\n",
    "        [\"humsavar\", \"Deleteriousness class\", columns_full],\n",
    "        [\"clinvar\", \"Deleteriousness class\", columns_full],\n",
    "        [\"cosmic\", \"Deleteriousness class\", columns_full],\n",
    "        [\"ab_bind\", \"ΔΔG\", [\"ddg_pred\", \"elaspic_score\", \"foldx_score\"]],\n",
    "        # [\"taipale\", \"ΔΔG\", eval_columns],\n",
    "        [\"skempi-v2\", \"ΔΔG (from affinity)\", [\"ddg_pred\", \"rosetta_complex_dg_change\"]],\n",
    "        # [\"skempi-v2\", \"ΔΔG (from Kon/Koff)\", [\"ddg_pred\", \"rosetta_complex_dg_change\"]],\n",
    "        [\"dunham_2020_tianyu\", \"Deep mutation scan\", [\"ddg_pred\", \"rosetta_complex_dg_change\"]],\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const_param = {\n",
    "    \"objective\": \"lambdarank\",\n",
    "    \"metric\": \"ndcg\",\n",
    "    \"verbosity\": -1,\n",
    "    \"eval_at\": 1_000_000,\n",
    "    \"label_gain\": [np.log2(2 + i) for i in range(0, 1_001)],\n",
    "    \"force_col_wise\": True,\n",
    "    \"num_threads\": 40,\n",
    "}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def objective(trial):\n",
    "    param = {\n",
    "        **const_param,\n",
    "        # num_trees = 100\n",
    "#         \"learning_rate\": trial.suggest_loguniform(\"lambda_l1\", 1e-3, 1.0),\n",
    "#         \"num_iterations\": trial.suggest_int(\"num_leaves\", 64, 256),\n",
    "        \"max_bin\": trial.suggest_categorical(\"max_bin\", [255, 511]),  # 255\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 512),  # 256\n",
    "        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 5, 200), # 100\n",
    "        \"lambda_l1\": trial.suggest_loguniform(\"lambda_l1\", 1e-8, 10.0),\n",
    "        \"lambda_l2\": trial.suggest_loguniform(\"lambda_l2\", 1e-8, 10.0),\n",
    "        \"feature_fraction\": trial.suggest_uniform(\"feature_fraction\", 0.4, 1.0),\n",
    "        \"bagging_fraction\": trial.suggest_uniform(\"bagging_fraction\", 0.4, 1.0),\n",
    "        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n",
    "    }\n",
    "\n",
    "    bsts = []\n",
    "    result_dfs = []\n",
    "    for train_df, test_df in train_test_splits:\n",
    "        assert not set(train_df[\"cluster_id\"]) & set(test_df[\"cluster_id\"])\n",
    "        bst = train_model((train_df, test_df), param)\n",
    "        bsts.append(bst)\n",
    "        \n",
    "        test_df = test_df.copy()\n",
    "        test_df[\"ddg_pred\"] = bst.predict(\n",
    "            test_df[feature_columns], num_iteration=bst.best_iteration\n",
    "        )\n",
    "        result_dfs.append(test_df)\n",
    "    result_df = pd.concat(result_dfs, ignore_index=True)\n",
    "    \n",
    "    score = get_aggregate_spearmanr(result_df, datasets_eval)\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=100, n_jobs=2)\n",
    "print(f\"Elaspsed: {time.perf_counter() - start_time}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if COI ==\"core\":\n",
    "    best_params = {'num_leaves': 131, 'lambda_l1': 0.06090843013079758, 'lambda_l2': 1.682306739340599, 'feature_fraction': 0.6427647079708247, 'bagging_fraction': 0.5908679308527225, 'bagging_freq': 6, 'min_child_samples': 47}\n",
    "else:\n",
    "    best_params = {\n",
    "        'max_bin': 511,\n",
    "        'num_leaves': 64,\n",
    "        'min_data_in_leaf': 168,\n",
    "        'lambda_l1': 1.8149466697376564e-05,\n",
    "        'lambda_l2': 4.3022548294881256e-07,\n",
    "        'feature_fraction': 0.6326206839855546,\n",
    "        'bagging_fraction': 0.7398095524057099,\n",
    "        'bagging_freq': 6,\n",
    "    }\n",
    "    \n",
    "best_params = {\"max_bin\": 255, \"learning_rate\": 0.1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    **const_param,\n",
    "    # **{\"max_bin\": 255, \"learning_rate\": 0.1, \"force_col_wise\": True},\n",
    "    **best_params,\n",
    "    # **study.best_params,\n",
    "    \"num_threads\": 80,\n",
    "    \"verbosity\": 1,\n",
    "}\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "bsts = []\n",
    "result_dfs = []\n",
    "for split_idx, (train_df, test_df) in enumerate(train_test_splits):\n",
    "    print(split_idx, len(train_df), len(test_df))\n",
    "\n",
    "    assert not set(train_df[\"cluster_id\"]) & set(test_df[\"cluster_id\"])\n",
    "    bst = train_model((train_df, test_df), param, early_stopping_rounds=10)\n",
    "    bsts.append(bst)\n",
    "\n",
    "    test_df = test_df.copy()\n",
    "    test_df[\"ddg_pred\"] = bst.predict(\n",
    "        test_df[feature_columns], num_iteration=bst.best_iteration\n",
    "    )\n",
    "    result_dfs.append(test_df)\n",
    "result_df = pd.concat(result_dfs, ignore_index=True)\n",
    "print(f\"Elaspsed: {time.perf_counter() - start_time}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = get_aggregate_spearmanr(result_df, datasets_eval)\n",
    "score\n",
    "# Interface: 0.325\n",
    "# Core: 0.3565635315814614"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "300:\n",
    "200: 0.3910785927589155\n",
    "100: 0.4002496796653158"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(f\"05_feature_elimination/feature-columns-{COI}-0.json\", \"wt\") as fout:\n",
    "    json.dump(feature_columns, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(f\"05_feature_elimination/feature-columns-interface-0.json\", \"rt\") as fin:\n",
    "    print(len(json.load(fin)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split_idx, bst in enumerate(tqdm(bsts, total=n_components)):\n",
    "    print(split_idx)\n",
    "\n",
    "    for column in pca_columns:\n",
    "        pickle_file = NOTEBOOK_DIR.joinpath(f\"pca-{column}-{COI}.pickle\")\n",
    "        pca = torch.load(pickle_file)\n",
    "\n",
    "        values = np.vstack(input_test_df[column].values)\n",
    "        values_out = pca.transform(values)\n",
    "        for i in range(n_components):\n",
    "            new_column = f\"{column}_{i}_pc\"\n",
    "            input_test_df[new_column] = values_out[:, i]\n",
    "\n",
    "    input_test_df[f\"ddg_pred_{split_idx}\"] = bst.predict(\n",
    "        input_test_df[feature_columns], num_iteration=bst.best_iteration\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_test_df[f\"ddg_pred\"] = input_test_df[[f\"ddg_pred_{i}\" for i in range(6)]].max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spearman_corrs_global(df, feature_columns, target_column, drop_na=True):\n",
    "    if drop_na:\n",
    "        df = df.dropna(subset=feature_columns + [target_column])\n",
    "    corrs = {}\n",
    "    for column in feature_columns:\n",
    "        sign = -1 if any(column.startswith(prefix) for prefix in [\"provean_\", \"protbert_\", \"proteinsolver_\"]) else 1\n",
    "        df_nna = df.dropna(subset=[column, target_column])\n",
    "        corr = stats.spearmanr(sign * df_nna[column], df_nna[target_column])\n",
    "        corrs[column] = (corr[0], corr[1], len(df_nna))\n",
    "        # print(f\"{column:30s} {corr[0]:+.4} {corr[1]:.4}\")\n",
    "    return corrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spearman_corrs_perseq(df, feature_columns, target_column, min_gp_size=6, drop_na=True):\n",
    "    if drop_na:\n",
    "        df = df.dropna(subset=feature_columns + [target_column])\n",
    "    results = {c: [] for c in feature_columns}\n",
    "    for _, gp in df.groupby(\"unique_id\"):\n",
    "        if len(gp) < min_gp_size or len(set(gp[target_column])) < 2:\n",
    "            continue\n",
    "        for column in feature_columns:\n",
    "            sign = -1 if any(column.startswith(prefix) for prefix in [\"provean_\", \"protbert_\", \"proteinsolver_\"]) else 1\n",
    "            gp_nna = gp.dropna(subset=[column, target_column])\n",
    "            corr = stats.spearmanr(sign * gp_nna[column], gp_nna[target_column])\n",
    "            results[column].append(corr[0])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_spearman_corrs(corrs):\n",
    "    for column, corr in corrs.items():\n",
    "        print(f\"{column:30s} {corr[0]:+.4} {corr[1]:.4} ({corr[2]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import set_matplotlib_formats\n",
    "\n",
    "set_matplotlib_formats(\"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIGURE_OUTPUT_DIR = Path(f\"05_model_validation_{COI}\").resolve()\n",
    "FIGURE_OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "FIGURE_OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = plt.cm.get_cmap(\"tab20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df[[\"dataset\", \"effect_type\"]].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spearman_corrs_global_xxx(df, feature_columns, target_column, drop_na=True):\n",
    "    if drop_na:\n",
    "        df = df.dropna(subset=feature_columns + [target_column])\n",
    "    corrs = {}\n",
    "    for column in feature_columns:\n",
    "        sign = -1 if any(column.startswith(prefix) for prefix in [\"provean_\", \"protbert_\", \"proteinsolver_\"]) else 1\n",
    "        df_nna = df.dropna(subset=[column, target_column])\n",
    "        feature_values = sign * df_nna[column].values\n",
    "        feature_values = np.hstack([feature_values, -feature_values])\n",
    "        target_values = df_nna[target_column]\n",
    "        target_values = np.hstack([target_values, -target_values])\n",
    "        corr = stats.spearmanr(feature_values, target_values)\n",
    "        corrs[column] = (corr[0], corr[1], len(df_nna))\n",
    "        # print(f\"{column:30s} {corr[0]:+.4} {corr[1]:.4}\")\n",
    "    return corrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev = [False]\n",
    "\n",
    "\n",
    "if rev == [False]:\n",
    "    suffix = \"\"\n",
    "else:\n",
    "    assert rev == [False, True]\n",
    "    suffix = \"-rev\" \n",
    "    \n",
    "    \n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "\n",
    "fg, axs = plt.subplots(2, len(datasets_eval), figsize=(12, 8))\n",
    "\n",
    "for idx, (dataset, effect_type, eval_columns) in enumerate(datasets_eval):\n",
    "    df = result_df[\n",
    "        (result_df[\"effect_type\"] == effect_type)\n",
    "        & (result_df[\"dataset\"] == dataset)\n",
    "        & (result_df[\"rev\"].isin(rev))\n",
    "    ]\n",
    "    \n",
    "    if dataset == \"skempi-v2\":\n",
    "        df = df[df[\"unique_id\"].isin(skempi_v2_unique_ids)]\n",
    "\n",
    "    corrs = get_spearman_corrs_global(df, eval_columns, \"effect\")\n",
    "    per_sequence_stats = get_spearman_corrs_perseq(df, eval_columns, \"effect\", min_gp_size=8)\n",
    "\n",
    "    ax = axs[0, idx]\n",
    "    x = np.arange(len(corrs))\n",
    "    y = [c[0] for c in corrs.values()]\n",
    "    out = ax.bar(x, y, color=cmap(1), edgecolor=\"k\")\n",
    "    _ = ax.set_xticks(x)\n",
    "    _ = ax.set_xticklabels([\"\"] * len(x), rotation=\"vertical\")\n",
    "    ax.set_title(f\"{dataset}\")\n",
    "    ax.set_ylim(-0.025, 0.825)\n",
    "    if idx == 0:\n",
    "        ax.set_ylabel(\"Global Spearman's ρ\")\n",
    "        ax.yaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
    "        \n",
    "    ax = axs[1, idx]\n",
    "    out = ax.boxplot(\n",
    "        per_sequence_stats.values(),\n",
    "        patch_artist=True,\n",
    "        boxprops={\"facecolor\": cmap(1)},\n",
    "        medianprops={\"color\": cmap(0)},\n",
    "    )\n",
    "    bp = ax.set_xticklabels(per_sequence_stats.keys(), rotation=\"vertical\")\n",
    "    ax.set_ylim(-1.05, 1.05)\n",
    "    if idx == 0:\n",
    "        ax.set_ylabel(\"Per-protein Spearman's ρ\")\n",
    "        ax.yaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
    "\n",
    "fg.subplots_adjust(top=0.95, right=0.98, bottom=0.38)\n",
    "fg.savefig(FIGURE_OUTPUT_DIR.joinpath(f\"corrs-xval-{COI}{suffix}.svg\"), dpi=300)\n",
    "fg.savefig(FIGURE_OUTPUT_DIR.joinpath(f\"corrs-xval-{COI}{suffix}.png\"), dpi=300)\n",
    "fg.savefig(FIGURE_OUTPUT_DIR.joinpath(f\"corrs-xval-{COI}{suffix}.pdf\"), dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df[df[\"rev\"] == False][\"effect\"], df[df[\"rev\"] == False][\"ddg_pred\"], 'r.', alpha=0.3)\n",
    "plt.plot(-df[df[\"rev\"] == False][\"effect\"], -df[df[\"rev\"] == False][\"ddg_pred\"], 'g.', alpha=0.3)\n",
    "plt.plot(df[df[\"rev\"] == True][\"effect\"], df[df[\"rev\"] == True][\"ddg_pred\"], 'b.', alpha=0.3)\n",
    "plt.xlabel(\"effect\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if COI == \"core\":\n",
    "    eval_columns = [\n",
    "        \"ddg_pred\",\n",
    "#         \"elaspic_score\",\n",
    "#         \"foldx_score\",\n",
    "#         \"rosetta_dg_change\",\n",
    "#         \"provean_score\",\n",
    "        \"protbert_core_score_change\",\n",
    "        \"proteinsolver_core_score_change\",\n",
    "    ]\n",
    "else:\n",
    "    eval_columns = [\n",
    "        \"ddg_pred\",\n",
    "#         \"elaspic_score\",\n",
    "#         \"foldx_score\",\n",
    "#         \"rosetta_complex_dg_change\",\n",
    "#         \"provean_score\",\n",
    "        \"protbert_core_score_change\",\n",
    "        \"proteinsolver_core_score_change\",\n",
    "        #\n",
    "#         \"rosetta_opt_apart_dg_change\",\n",
    "#         \"rosetta_apart_dg_change\",\n",
    "#         \"rosetta_opt_bind_dg_change\",\n",
    "#         \"rosetta_bind_dg_change\",\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, effect_type = (\"huang_2020\", \"ΔΔG\")\n",
    "dataset, effect_type = (\"starr_2020_tianyu\", \"Deep mutation scan\")\n",
    "# dataset, effect_type = (\"cagi5_frataxin\", \"ΔΔG\")\n",
    "\n",
    "rev = [False, True]\n",
    "\n",
    "# df = result_df[\n",
    "#     (result_df[\"effect_type\"] == effect_type)\n",
    "#     & (result_df[\"dataset\"] == dataset)\n",
    "#     & (result_df[\"rev\"].isin(rev))\n",
    "# ]\n",
    "\n",
    "df = input_test_df[\n",
    "    (input_test_df[\"effect_type\"] == effect_type)\n",
    "    & (input_test_df[\"dataset\"] == dataset)\n",
    "    & (input_test_df[\"rev\"].isin(rev))\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "suffix = f\"-{dataset}\"\n",
    "if rev != [False, True]:\n",
    "    assert rev == [False]\n",
    "    suffix += \"-norev\"\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "corrs = get_spearman_corrs_global(df, eval_columns, \"effect\")\n",
    "per_sequence_stats = get_spearman_corrs_perseq(df, eval_columns, \"effect\", min_gp_size=6)\n",
    "\n",
    "fg, axs = plt.subplots(2, 1, figsize=(3, 8))\n",
    "\n",
    "ax = axs[0]\n",
    "x = np.arange(len(corrs))\n",
    "y = [c[0] for c in corrs.values()]\n",
    "out = ax.bar(x, y, color=cmap(1), edgecolor=\"k\")\n",
    "_ = ax.set_xticks(x)\n",
    "_ = ax.set_xticklabels([\"\"] * len(x), rotation=\"vertical\")\n",
    "ax.set_ylabel(\"Global Spearman's ρ\")\n",
    "ax.set_title(f\"{dataset} - {effect_type}\")\n",
    "ax.yaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
    "\n",
    "ax = axs[1]\n",
    "out = ax.boxplot(\n",
    "    per_sequence_stats.values(),\n",
    "    patch_artist=True,\n",
    "    boxprops={\"facecolor\": cmap(1)},\n",
    "    medianprops={\"color\": cmap(0)},\n",
    ")\n",
    "bp = ax.set_xticklabels(per_sequence_stats.keys(), rotation=\"vertical\")\n",
    "ax.set_ylabel(\"Per-protein Spearman's ρ\")\n",
    "ax.yaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
    "\n",
    "fg.subplots_adjust(top=0.95, right=0.98, bottom=0.38)\n",
    "fg.savefig(FIGURE_OUTPUT_DIR.joinpath(f\"corrs-perseq{suffix}.svg\"), dpi=300)\n",
    "fg.savefig(FIGURE_OUTPUT_DIR.joinpath(f\"corrs-perseq{suffix}.png\"), dpi=300)\n",
    "fg.savefig(FIGURE_OUTPUT_DIR.joinpath(f\"corrs-perseq{suffix}.pdf\"), dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df[df[\"rev\"] == False][\"effect\"], df[df[\"rev\"] == False][\"ddg_pred\"], 'r.', alpha=0.3)\n",
    "# plt.plot(-df[df[\"rev\"] == False][\"effect\"], -df[df[\"rev\"] == False][\"ddg_pred\"], 'g.', alpha=0.3)\n",
    "# plt.plot(df[df[\"rev\"] == True][\"effect\"], df[df[\"rev\"] == True][\"ddg_pred\"], 'b.', alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df[df[\"rev\"] == False][[\"mutation\", \"ddg_pred\"]]\n",
    "df2[\"mutation\"] = df2[\"mutation\"].str[-1] + df2[\"mutation\"].str[1:-1] + df2[\"mutation\"].str[0]\n",
    "df2[\"ddg_pred\"] = -df2[\"ddg_pred\"]\n",
    "df2 = df2.merge(df[df[\"rev\"] == True][[\"mutation\", \"ddg_pred\"]], on=[\"mutation\"])\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.spearmanr(df2[\"ddg_pred_x\"], df2[\"ddg_pred_y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df[df[\"rev\"] == False][\"effect\"], df[df[\"rev\"] == False][\"ddg_pred\"], 'r.', alpha=0.3)\n",
    "plt.plot(df[df[\"rev\"] == True][\"effect\"], df[df[\"rev\"] == True][\"ddg_pred\"], 'b.', alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df[df[\"rev\"] == False][\"effect\"], bins=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_feval(preds, train_data):\n",
    "    labels = train_data.get_label()\n",
    "    groups = train_data.get_group()\n",
    "    \n",
    "    if len(set(preds)) < 2 or len(set(labels)) < 2:\n",
    "        global_corr = 0\n",
    "    else:\n",
    "        global_corr = stats.spearmanr(preds, labels)[0]\n",
    "    \n",
    "    weighted_corr_total = 0\n",
    "    weight_total = 0\n",
    "    start = 0\n",
    "    for group in groups:\n",
    "        stop = start + group\n",
    "        preds_slice = preds[start:stop]\n",
    "        labels_slice = labels[start:stop]\n",
    "        start = stop\n",
    "\n",
    "        weight = math.sqrt(group)\n",
    "        if group < 2:\n",
    "            continue\n",
    "        elif len(set(labels_slice)) < 2:\n",
    "            continue\n",
    "        elif len(set(preds_slice)) < 2:\n",
    "            group_corr = 0\n",
    "        else:\n",
    "            group_corr =  stats.spearmanr(preds_slice, labels_slice)[0]\n",
    "        weighted_corr_total += weight * group_corr\n",
    "        weight_total += weight\n",
    "    assert start == sum(groups)\n",
    "    pergroup_corr = weighted_corr_total / weight_total\n",
    "        \n",
    "    eval_name = \"wavg_spearman_rho\"\n",
    "    # eval_result = (global_corr / pergroup_corr) / 2\n",
    "    eval_result = pergroup_corr\n",
    "    is_higher_better = True\n",
    "    return eval_name, eval_result, is_higher_better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_score(df):\n",
    "    corr_global = stats.spearmanr(df[\"ddg_pred\"], df[\"effect\"])[0]\n",
    "    \n",
    "    perseq_score = 0\n",
    "    perseq_weight = 0\n",
    "    for _, gp in df.groupby(\"unique_id\"):\n",
    "        if len(set(gp[\"effect\"])) < 2:\n",
    "            continue\n",
    "        elif len(set(gp[\"ddg_pred\"])) < 2:\n",
    "            weight = math.sqrt(len(gp))\n",
    "            corr = 0\n",
    "        else:\n",
    "            weight = math.sqrt(len(gp))\n",
    "            corr = stats.spearmanr(gp[\"ddg_pred\"], gp[\"effect\"])[0]\n",
    "        perseq_score += corr * weight\n",
    "        perseq_weight += weight\n",
    "    corr_perseq = perseq_score / perseq_weight\n",
    "    \n",
    "    return (corr_global + corr_perseq) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = result_df[\n",
    "    (result_df[\"effect_type\"] == \"ΔΔG\")\n",
    "    & (result_df[\"dataset\"] == \"skempi++\")\n",
    "    & (result_df[\"rev\"].isin([False]))\n",
    "]\n",
    "\n",
    "corrs = get_spearman_corrs_global(df, eval_columns, \"effect\")\n",
    "fg, ax = plt.subplots()\n",
    "x = np.arange(len(corrs))\n",
    "y = [c[0] for c in corrs.values()]\n",
    "out = ax.bar(x, y, color=cmap(1), edgecolor=\"k\")\n",
    "_ = ax.set_xticks(x)\n",
    "_ = ax.set_xticklabels(corrs.keys(), rotation=\"vertical\")\n",
    "ax.set_ylabel(\"Spearman's ρ\")\n",
    "ax.set_title(\"Global correlations\")\n",
    "fg.savefig(FIGURE_OUTPUT_DIR.joinpath(\"corrs-global-skempi-norev.svg\"), dpi=300)\n",
    "fg.savefig(FIGURE_OUTPUT_DIR.joinpath(\"corrs-global-skempi.png\"), dpi=300)\n",
    "fg.savefig(FIGURE_OUTPUT_DIR.joinpath(\"corrs-global-skempi.pdf\"), dpi=300)\n",
    "\n",
    "per_sequence_stats = get_spearman_corrs_perseq(result_df, eval_columns, \"effect\", min_gp_size=6)\n",
    "fg, ax = plt.subplots()\n",
    "out = ax.boxplot(\n",
    "    per_sequence_stats.values(),\n",
    "    patch_artist=True,\n",
    "    boxprops={\"facecolor\": cmap(1)},\n",
    "    medianprops={\"color\": cmap(0)},\n",
    ")\n",
    "bp = ax.set_xticklabels(per_sequence_stats.keys(), rotation=\"vertical\")\n",
    "ax.set_ylabel(\"Spearman's ρ\")\n",
    "ax.set_title(\"Per-protein correlations\")\n",
    "fg.savefig(FIGURE_OUTPUT_DIR.joinpath(\"corrs-perseq-skempi.svg\"), dpi=300)\n",
    "fg.savefig(FIGURE_OUTPUT_DIR.joinpath(\"corrs-perseq-skempi.png\"), dpi=300)\n",
    "fg.savefig(FIGURE_OUTPUT_DIR.joinpath(\"corrs-perseq-skempi.pdf\"), dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_spearman_stats(\n",
    "    result_df[\n",
    "        (result_df[\"effect_type\"] == \"Deleteriousness class\")\n",
    "        & (result_df[\"rev\"].isin([True, False]))\n",
    "    ],\n",
    "    eval_columns,\n",
    "    \"effect\",\n",
    ")\n",
    "# 0.488"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df[\n",
    "    (result_df[\"effect_type\"] == \"Deleteriousness class\") & (result_df[\"rev\"].isin([True, False]))\n",
    "][\"dataset\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_spearman_stats(\n",
    "    result_df[\n",
    "        (result_df[\"effect_type\"] == \"Deleteriousness score\")\n",
    "        & (result_df[\"rev\"].isin([True, False]))\n",
    "    ],\n",
    "    eval_columns,\n",
    "    \"effect\",\n",
    ")\n",
    "# 0.4128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_spearman_stats(result_df, [\"ddg_pred\", \"rosetta_dg_change\"], \"label\")  # 0.4646"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_spearman_stats(result_df[result_df[\"effect_type\"] == \"Deleteriousness score\"], eval_columns, \"label\")  # 0.4077"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_spearman_stats(result_df[result_df[\"effect_type\"] == \"ΔΔG\"], eval_columns, \"effect\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_per_sequence_stats(df, feature_columns, target_column, min_gp_size=6):\n",
    "    df = df.dropna(subset=feature_columns + [target_column])\n",
    "    results = {c: [] for c in feature_columns}\n",
    "    for _, gp in df.groupby(\"unique_id\"):\n",
    "        if len(gp) < min_gp_size or len(set(gp[target_column])) < 2:\n",
    "            continue\n",
    "        for column in feature_columns:\n",
    "            corr = stats.spearmanr(gp[column], gp[target_column])\n",
    "            results[column].append(corr[0])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_sequence_stats = compute_per_sequence_stats(result_df, eval_columns, \"effect\", 6)\n",
    "\n",
    "fg, ax = plt.subplots()\n",
    "\n",
    "out = ax.boxplot(per_sequence_stats.values())\n",
    "_ = ax.set_xticklabels(per_sequence_stats.keys(), rotation=\"vertical\")\n",
    "# ax.set_ylim(-1, 1)\n",
    "# fg.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_sequence_stats_ddg = compute_per_sequence_stats(\n",
    "    result_df[result_df[\"effect_type\"] == \"Deleteriousness class\"], eval_columns, \"effect\", 18\n",
    ")\n",
    "\n",
    "fg, ax = plt.subplots()\n",
    "\n",
    "out = ax.boxplot(per_sequence_stats_ddg.values())\n",
    "_ = ax.set_xticklabels(per_sequence_stats_ddg.keys(), rotation=\"vertical\")\n",
    "# ax.set_ylim(-1, 1)\n",
    "# fg.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_sequence_stats_ddg = compute_per_sequence_stats(\n",
    "    result_df[result_df[\"effect_type\"] == \"Deleteriousness score\"], eval_columns, \"effect\", 18\n",
    ")\n",
    "\n",
    "fg, ax = plt.subplots()\n",
    "\n",
    "out = ax.boxplot(per_sequence_stats_ddg.values())\n",
    "_ = ax.set_xticklabels(per_sequence_stats_ddg.keys(), rotation=\"vertical\")\n",
    "# ax.set_ylim(-1, 1)\n",
    "# fg.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = [\"r\", \"g\", \"b\", \"y\"]\n",
    "for x, val, c in zip(xs, vals, palette):\n",
    "    plt.scatter(x, val, alpha=0.4, color=c)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[(train_df[\"effect\"] * 1_000).astype(np.int) > 300_000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "_ = plt.hist(input_df[\"effect\"], bins=100, range=(-5, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    \"objective\": \"lambdarank\",\n",
    "    \"metric\": \"ndcg\",\n",
    "    \"ndcg_eval_at\": 1000000000000,\n",
    "    \"max_bin\": 255,\n",
    "}\n",
    "\n",
    "\n",
    "bst = lgb.train(param, train_ds, num_boost_round=100, valid_sets=[valid_ds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = bst.predict(test_df.drop(columns_to_drop, axis=1), num_iteration=bst.best_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = bst.predict(test_df.drop(columns_to_drop, axis=1), num_iteration=bst.best_iteration)\n",
    "test_df = test_df.copy()\n",
    "test_df[\"ddg_pred\"] = ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.spearmanr(test_df[\"effect\"], test_df[\"ddg_pred\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.spearmanr(test_df[\"effect\"], test_df[\"foldx_score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.spearmanr(test_df[\"effect\"], test_df[\"provean_score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
