{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Calculate features using [ProtBert](https://github.com/agemagician/ProtTrans).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import os\n",
    "import re\n",
    "import socket\n",
    "import subprocess\n",
    "import sys\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "import kmbio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from elaspic2.plugins.protbert import (\n",
    "    ProtBert,\n",
    "    ProtBertAnalyzeError,\n",
    "    ProtBertBuildError,\n",
    ")\n",
    "from kmbio import PDB\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ProtBert.load_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTEBOOK_DIR = Path(\"02_run_protbert\").resolve()\n",
    "NOTEBOOK_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "NOTEBOOK_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"DATAPKG_OUTPUT_DIR\" in os.environ:\n",
    "    OUTPUT_DIR = Path(os.getenv(\"DATAPKG_OUTPUT_DIR\")).joinpath(\"elaspic2\").resolve()\n",
    "else:\n",
    "    OUTPUT_DIR = NOTEBOOK_DIR.parent\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (slurm_tmpdir := os.getenv(\"SLURM_TMPDIR\")) is not None:\n",
    "    os.environ[\"TMPDIR\"] = slurm_tmpdir\n",
    "    \n",
    "print(tempfile.gettempdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"scinet\" in socket.gethostname():\n",
    "    CPU_COUNT = 40\n",
    "else:\n",
    "    CPU_COUNT = max(1, len(os.sched_getaffinity(0)))\n",
    "\n",
    "CPU_COUNT = CPU_COUNT // 8\n",
    "\n",
    "CPU_COUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = os.getenv(\"DATASET_NAME\")\n",
    "TASK_ID = os.getenv(\"SLURM_ARRAY_TASK_ID\")\n",
    "TASK_COUNT = os.getenv(\"ORIGINAL_ARRAY_TASK_COUNT\") or os.getenv(\"SLURM_ARRAY_TASK_COUNT\")\n",
    "\n",
    "TASK_ID = int(TASK_ID) if TASK_ID is not None else None\n",
    "TASK_COUNT = int(TASK_COUNT) if TASK_COUNT is not None else None\n",
    "\n",
    "DATASET_NAME, TASK_ID, TASK_COUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = TASK_ID is None\n",
    "\n",
    "if DEBUG:\n",
    "    DATASET_NAME = \"elaspic-training-set-core\"\n",
    "    TASK_ID = 103\n",
    "    TASK_COUNT = 162\n",
    "else:\n",
    "    assert DATASET_NAME is not None\n",
    "    assert TASK_ID is not None\n",
    "    assert TASK_COUNT is not None\n",
    "\n",
    "DATASET_NAME, TASK_ID, TASK_COUNT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = OUTPUT_DIR.joinpath(\"01_load_data\", f\"{DATASET_NAME}.parquet\")\n",
    "\n",
    "input_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfile = pq.ParquetFile(input_file)\n",
    "\n",
    "pfile.num_row_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert TASK_COUNT == pfile.num_row_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DF = pfile.read_row_group(TASK_ID - 1).to_pandas(integer_object_nulls=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(INPUT_DF.head(2))\n",
    "print(len(INPUT_DF))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = OUTPUT_DIR.joinpath(NOTEBOOK_DIR.name)\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = []\n",
    "for row in tqdm(INPUT_DF.itertuples(), total=len(INPUT_DF)):\n",
    "\n",
    "    data_core = ProtBert.build(row.protein_sequence, None)\n",
    "    if row.ligand_sequence is not None:\n",
    "        data_interface = ProtBert.build(row.protein_sequence, row.ligand_sequence)\n",
    "    else:\n",
    "        data_interface = None\n",
    "\n",
    "    _seen = set()\n",
    "    for idx in range(len(row.mutation)):\n",
    "        mutation = row.mutation[idx]\n",
    "        if mutation in _seen:\n",
    "            print(\n",
    "                f\"Already added mutation '{mutation}' for protein ({row.unique_id}, {row.dataset}, {row.name}).\"\n",
    "            )\n",
    "            continue\n",
    "        _seen.add(mutation)\n",
    "\n",
    "        aa = \"GVALICMFWPDESTYQNKRH\"\n",
    "        if re.search(f\"^[{aa}][1-9]+[0-9]*[{aa}]$\", mutation) is None:\n",
    "            print(f\"Skipping mutation {mutation} because it appears to be malformed.\")\n",
    "            continue\n",
    "\n",
    "        if mutation[0] == mutation[-1]:\n",
    "            print(f\"Skipping mutation {mutation} because the wildtype and mutant residues are the same.\")\n",
    "            continue\n",
    "\n",
    "        data_mut = {\n",
    "            \"unique_id\": row.unique_id,\n",
    "            \"mutation\": row.mutation[idx],\n",
    "            \"effect\": row.effect[idx],\n",
    "            \"rev\": False,\n",
    "        }\n",
    "        tasks.append((data_core, data_interface, data_mut))\n",
    "\n",
    "len(tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate mutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker(input):\n",
    "    data_core, data_interface, data_mut = input\n",
    "    mutation = data_mut[\"mutation\"]\n",
    "    try:\n",
    "        results_core = ProtBert.analyze_mutation(f\"A_{mutation}\", data_core)\n",
    "        if data_interface is not None:\n",
    "            results_interface = ProtBert.analyze_mutation(f\"A_{mutation}\", data_interface)\n",
    "        else:\n",
    "            results_interface = {}\n",
    "    except ProtBertAnalyzeError as e:\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "    results = {\n",
    "        **data_mut,\n",
    "        **{f\"protbert_core_{key}\": value for key, value in results_core.items()},\n",
    "        **{f\"protbert_interface_{key}\": value for key, value in results_interface.items()},\n",
    "    }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = worker(tasks[647])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# _ = plt.hist(\n",
    "#     np.array(result[\"protbert_core_features_residue_wt\"])\n",
    "#     - np.array(result[\"protbert_core_features_residue_mut\"]),\n",
    "#     label=\"Residue wt → mut\",\n",
    "#     alpha=0.5,\n",
    "#     bins=100,\n",
    "#     range=(-0.1, 0.2),\n",
    "# )\n",
    "# _ = plt.hist(\n",
    "#     np.array(result[\"protbert_core_features_residue_wt\"])\n",
    "#     - np.array(result[\"protbert_interface_features_residue_wt\"]),\n",
    "#     label=\"Residue core → interface\",\n",
    "#     alpha=0.5,\n",
    "#     bins=100,\n",
    "#     range=(-0.1, 0.2),\n",
    "# )\n",
    "# _ = plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# _ = plt.hist(\n",
    "#     np.array(result[\"protbert_core_features_protein_wt\"])\n",
    "#     - np.array(result[\"protbert_core_features_protein_mut\"]),\n",
    "#     label=\"Protein wt → mut\",\n",
    "#     alpha=0.5,\n",
    "#     bins=100,\n",
    "#     range=(-0.1, 0.2),\n",
    "# )\n",
    "# _ = plt.hist(\n",
    "#     np.array(result[\"protbert_core_features_protein_wt\"])\n",
    "#     - np.array(result[\"protbert_interface_features_protein_wt\"]),\n",
    "#     label=\"Protein core → interface\",\n",
    "#     alpha=0.5,\n",
    "#     bins=100,\n",
    "#     range=(-0.1, 0.2),\n",
    "# )\n",
    "# _ = plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with concurrent.futures.ProcessPoolExecutor(CPU_COUNT) as pool:\n",
    "#     results = list(tqdm(pool.map(worker, tasks), total=len(tasks)))\n",
    "\n",
    "# results_df = pd.DataFrame([l for l in results if l is not None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = list(tqdm((worker(task) for task in tasks), total=len(tasks)))\n",
    "\n",
    "results_df = pd.DataFrame([l for l in results if l is not None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = output_dir.joinpath(f\"{DATASET_NAME}-{TASK_ID}-{TASK_COUNT}.parquet\")\n",
    "\n",
    "output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pq.write_table(pa.Table.from_pandas(results_df, preserve_index=False), output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
