{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shlex\n",
    "import subprocess\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "import lightgbm\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import torch\n",
    "from scipy import stats\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"max_columns\", 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paramters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTEBOOK_DIR = Path(\"04_train_model_core_norosetta\").resolve()\n",
    "NOTEBOOK_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "NOTEBOOK_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COI = \"core\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"DATAPKG_OUTPUT_DIR\" in os.environ:\n",
    "    OUTPUT_DIR = Path(os.getenv(\"DATAPKG_OUTPUT_DIR\")).joinpath(\"elaspic-v2\").resolve()\n",
    "else:\n",
    "    OUTPUT_DIR = NOTEBOOK_DIR.parent\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (slurm_tmpdir := os.getenv(\"SLURM_TMPDIR\")) is not None:\n",
    "    os.environ[\"TMPDIR\"] = slurm_tmpdir\n",
    "\n",
    "print(tempfile.gettempdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if COI == \"core\":\n",
    "    datasets = [\n",
    "        \"elaspic-training-set-core\",\n",
    "        \"protherm-dagger-core\",\n",
    "        \"rocklin-2017-core\",\n",
    "        \"dunham-2020-core\",\n",
    "        \"starr-2020-core\",\n",
    "        \"cagi5-frataxin-core\",\n",
    "        \"huang-2020-core\",\n",
    "    ]\n",
    "else:\n",
    "    assert COI == \"interface\"\n",
    "    datasets = [\n",
    "        \"elaspic-training-set-interface\",\n",
    "        #     \"skempi-v2-interface\",\n",
    "        #     \"intact-mutations-interface\",\n",
    "        #     \"dunham-2020-interface\",\n",
    "        #     \"starr-2020-interface\",\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_generators = [\n",
    "    \"02_run_rosetta_ddg\",\n",
    "    \"02_run_proteinsolver\",\n",
    "    \"02_run_protbert\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_mutations(df):\n",
    "    results = []\n",
    "    for row in df.itertuples():\n",
    "        for idx in range(len(row.mutation)):\n",
    "            row_mut = {\n",
    "                \"unique_id\": row.unique_id,\n",
    "                \"dataset\": row.dataset,\n",
    "                \"name\": row.name,\n",
    "                \"mutation\": row.mutation[idx],\n",
    "                \"effect\": row.effect[idx],\n",
    "                \"effect_type\": row.effect_type,\n",
    "            }\n",
    "            for column in [\"provean_score\", \"foldx_score\", \"elaspic_score\"]:\n",
    "                if hasattr(row, column):\n",
    "                    row_mut[column] = getattr(row, column)[idx]\n",
    "            results.append(row_mut)\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_mutation_complement(df):\n",
    "    df = df.copy()\n",
    "    df[\"rev\"] = False\n",
    "\n",
    "    df_comp = df.copy()\n",
    "    df_comp[\"rev\"] = True\n",
    "    df_comp[\"mutation\"] = (\n",
    "        df_comp[\"mutation\"].str[-1] + df_comp[\"mutation\"].str[1:-1] + df_comp[\"mutation\"].str[0]\n",
    "    )\n",
    "    for column in [\"effect\", \"provean_score\", \"foldx_score\", \"elaspic_score\"]:\n",
    "        if column in df_comp:\n",
    "            df_comp[column] = -df_comp[column]\n",
    "    for column in df_comp:\n",
    "        if column.endswith(\"_wt\"):\n",
    "            column_mut = column[:-3] + \"_mut\"\n",
    "            df_comp[column], df_comp[column_mut] = (\n",
    "                df_comp[column_mut].copy(),\n",
    "                df_comp[column].copy(),\n",
    "            )\n",
    "\n",
    "    df_out = pd.concat([df, df_comp], ignore_index=True)\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df = pd.DataFrame(\n",
    "    [[0, \"M1A\", 1.234, \"wt score\", \"mut score\"], [1, \"M2C\", -0.05, \"wt score 2\", \"mut score 2\"]],\n",
    "    columns=[\"unique_id\", \"mutation\", \"effect\", \"feature_wt\", \"feature_mut\"],\n",
    ")\n",
    "\n",
    "tmp2_df = add_mutation_complement(tmp_df)\n",
    "\n",
    "display(tmp_df)\n",
    "display(tmp2_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_feature_dfs(feature_dfs):\n",
    "    def _clean_df(df):\n",
    "        df = df.copy()\n",
    "        assert len(df) == len(df[[\"unique_id\", \"mutation\"]].drop_duplicates())\n",
    "        for column in [\"effect\", \"effect_type\", \"provean_score\", \"foldx_score\", \"elaspic_score\"]:\n",
    "            if column in df:\n",
    "                del df[column]\n",
    "        return df\n",
    "\n",
    "    if not feature_dfs:\n",
    "        return None\n",
    "\n",
    "    df = _clean_df(feature_dfs[0])\n",
    "    for other_df in feature_dfs[1:]:\n",
    "        df = df.merge(\n",
    "            _clean_df(other_df), how=\"outer\", on=[\"unique_id\", \"mutation\", \"rev\"]\n",
    "        )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = {}\n",
    "for dataset_name in datasets:\n",
    "    input_file = OUTPUT_DIR.joinpath(\"01_load_data\", f\"{dataset_name}.parquet\")\n",
    "    pfile = pq.ParquetFile(input_file)\n",
    "    task_count = pfile.num_row_groups\n",
    "    df = pfile.read().to_pandas(integer_object_nulls=True)\n",
    "    expanded_df = (\n",
    "        add_mutation_complement(expand_mutations(df))\n",
    "        .drop_duplicates(subset=[\"unique_id\", \"mutation\"])\n",
    "        .sort_values([\"unique_id\", \"mutation\"])\n",
    "    )\n",
    "    sequence_df = df[[\"unique_id\", \"protein_sequence\", \"ligand_sequence\"]].drop_duplicates()\n",
    "\n",
    "    features = {}\n",
    "    for feature_generator in feature_generators:\n",
    "        output_dir = OUTPUT_DIR.joinpath(feature_generator)\n",
    "        feature_dfs = []\n",
    "        for task_id in range(1, task_count + 1):\n",
    "            if feature_generator in [\"02_run_rosetta_ddg\"]:\n",
    "                # wt → mut\n",
    "                output_file_wt2mut = output_dir.joinpath(\n",
    "                    f\"{dataset_name}-wt2mut-{task_id}-{task_count}.parquet\"\n",
    "                )\n",
    "                if not output_file_wt2mut.is_file():\n",
    "                    print(f\"File {output_file_wt2mut} is missing. Skipping...\")\n",
    "                    continue\n",
    "                feature_wt2mut_df = pq.read_table(output_file_wt2mut).to_pandas(\n",
    "                    integer_object_nulls=True\n",
    "                )\n",
    "                feature_wt2mut_df[\"rev\"] = False\n",
    "                feature_dfs.append(feature_wt2mut_df)\n",
    "\n",
    "                # mut → wt\n",
    "                output_file_mut2wt = output_dir.joinpath(\n",
    "                    f\"{dataset_name}-mut2wt-{task_id}-{task_count}.parquet\"\n",
    "                )\n",
    "                if not output_file_mut2wt.is_file():\n",
    "                    print(f\"File {output_file_mut2wt} is missing. Skipping...\")\n",
    "                    continue\n",
    "                feature_mut2wt_df = pq.read_table(output_file_mut2wt).to_pandas(\n",
    "                    integer_object_nulls=True\n",
    "                )\n",
    "                assert feature_mut2wt_df[\"unique_id\"].min() < 0\n",
    "                feature_mut2wt_df[\"unique_id\"] = -feature_mut2wt_df[\"unique_id\"]\n",
    "                feature_mut2wt_df[\"rev\"] = True\n",
    "                feature_dfs.append(feature_mut2wt_df)\n",
    "            else:\n",
    "                output_file = output_dir.joinpath(f\"{dataset_name}-{task_id}-{task_count}.parquet\")\n",
    "                if not output_file.is_file():\n",
    "                    print(f\"File {output_file} is missing. Skipping...\")\n",
    "                    continue\n",
    "                feature_df = pq.read_table(output_file).to_pandas(integer_object_nulls=True)\n",
    "                feature_df = add_mutation_complement(feature_df)\n",
    "                feature_dfs.append(feature_df)\n",
    "\n",
    "        if not feature_dfs:\n",
    "            print(\n",
    "                f\"No data collected for dataset {dataset_name} and feature generator {feature_generator}.\"\n",
    "            )\n",
    "            continue\n",
    "        feature_df = pd.concat(feature_dfs, ignore_index=True)\n",
    "        features[feature_generator] = feature_df\n",
    "    input_data[dataset_name] = {\n",
    "        \"expanded_df\": expanded_df,\n",
    "        \"sequence_df\": sequence_df,\n",
    "        \"feature_df\": merge_feature_dfs(features),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_df = pd.concat(\n",
    "    [d[\"expanded_df\"] for d in input_data.values() if d[\"feature_df\"] is not None]\n",
    ")\n",
    "\n",
    "sequence_df = pd.concat(\n",
    "    [d[\"sequence_df\"] for d in input_data.values() if d[\"feature_df\"] is not None]\n",
    ")\n",
    "\n",
    "features_df = pd.concat(\n",
    "    [d[\"feature_df\"] for d in input_data.values() if d[\"feature_df\"] is not None]\n",
    ").sort_values([\"unique_id\", \"mutation\"])\n",
    "assert features_df[\"unique_id\"].min() >= 0\n",
    "len(features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_nn_df = features_df.dropna(subset=[c for c in features_df if not c.startswith(\"rosetta_\")])\n",
    "print(\n",
    "    f\"Lost {len(features_df) - len(features_nn_df):,} out of {len(features_df):,} rows due to missing values.\"\n",
    ")\n",
    "\n",
    "len(features_nn_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df = expanded_df.merge(features_nn_df, on=[\"unique_id\", \"mutation\", \"rev\"], validate=\"1:1\")\n",
    "assert len(input_df) == len(features_nn_df)\n",
    "print(\n",
    "    f\"Lost {len(expanded_df) - len(input_df):,} out of {len(expanded_df):,} rows due to missing features.\"\n",
    ")\n",
    "\n",
    "# Correct the sign on some features\n",
    "for dataset, effect_type in [\n",
    "#     (\"cagi4_sumo_ligase\", \"Deleteriousness score\"),\n",
    "#     (\"benedix_et_al\", \"ΔΔG\"),\n",
    "#     (\"hiv_escape_mutations\", \"ΔΔG\"),\n",
    "]:\n",
    "    mask = (input_df[\"dataset\"] == dataset) & (input_df[\"effect_type\"] == effect_type)\n",
    "    if mask.any():\n",
    "        print(dataset, effect_type)\n",
    "        input_df.loc[mask, \"effect\"] = -input_df.loc[mask, \"effect\"]\n",
    "\n",
    "len(input_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if COI == \"core\":\n",
    "    datasets_to_drop = {\n",
    "        \"cagi4_sumo_ligase\",\n",
    "        \"benedix_et_al\",\n",
    "        \"hiv_escape_mutations\",\n",
    "        \"ab_bind\",\n",
    "        \"skempiskempi\",\n",
    "        \"taipale_ppi\",\n",
    "    }\n",
    "\n",
    "input_df = input_df[~input_df[\"dataset\"].isin(datasets_to_drop)]\n",
    "\n",
    "len(input_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(input_df.head())\n",
    "print(len(input_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not input_df[\"foldx_score\"].isnull().any()\n",
    "assert not input_df[\"effect\"].isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df[\"effect_type\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df[\"dataset\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_columns = []\n",
    "\n",
    "for column in list(input_df):\n",
    "    if column.endswith(\"_mut\"):\n",
    "        print(column)\n",
    "        column_wt = column[:-4] + \"_wt\"\n",
    "        column_change = column[:-4] + \"_change\"\n",
    "        value_sample = input_df[column].iloc[0]\n",
    "        if isinstance(value_sample, (list, np.ndarray)):\n",
    "            input_df[column_change] = input_df[column] - input_df[column_wt]\n",
    "            pca_columns.extend([column_wt, column, column_change])\n",
    "        else:\n",
    "            input_df[column_change] = input_df[column] - input_df[column_wt]\n",
    "        # del input_df[column]\n",
    "\n",
    "pca_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (dataset, effect_type), gp in input_df.groupby([\"dataset\", \"effect_type\"]):\n",
    "    gp = gp.copy()\n",
    "    gp_sub = gp.dropna(subset=[\"effect\", \"protbert_core_score_change\"])\n",
    "    corr1 = stats.spearmanr(gp_sub[\"effect\"], gp_sub[\"protbert_core_score_change\"])\n",
    "    gp_sub = gp_sub[gp_sub[\"rev\"] == False]\n",
    "    corr2 = stats.spearmanr(gp_sub[\"effect\"], gp_sub[\"protbert_core_score_change\"])\n",
    "    if corr1[0] > 0 or corr2[0] > 0:\n",
    "        print(dataset, effect_type)\n",
    "        for column in [\n",
    "            \"provean_score\",\n",
    "            \"foldx_score\",\n",
    "            \"elaspic_score\",\n",
    "            \"rosetta_dg_change\",\n",
    "            \"protbert_core_score_change\",\n",
    "            \"proteinsolver_core_score_change\",\n",
    "        ]:\n",
    "            gp_sub = gp.dropna(subset=[\"effect\", column])\n",
    "            corr = stats.spearmanr(gp_sub[\"effect\"], gp_sub[column])\n",
    "            print(f\"{column:30s} {corr[0]:+.4} {corr[1]:.4}\")\n",
    "            gp_sub = gp_sub[gp_sub[\"rev\"] == False]\n",
    "            corr = stats.spearmanr(gp_sub[\"effect\"], gp_sub[column])\n",
    "            print(f\"{column:30s} {corr[0]:+.4} {corr[1]:.4}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (dataset, effect_type), gp in input_df.groupby([\"dataset\", \"effect_type\"]):\n",
    "    gp = gp.dropna(subset=[\"effect\", \"provean_score\"])\n",
    "    assert len(gp)\n",
    "    corr = stats.spearmanr(gp[\"effect\"], gp[\"provean_score\"])\n",
    "    assert corr[0] <= 0, (dataset, effect_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = input_df[\n",
    "    (input_df[\"effect_type\"] == \"ΔΔG\")\n",
    "    & (input_df[\"dataset\"] == \"hiv_escape_mutations\")\n",
    "    & (input_df[\"rev\"] == False)\n",
    "].dropna()\n",
    "\n",
    "df = input_df[\n",
    "#     (input_df[\"effect_type\"] == \"Deleteriousness score\")\n",
    "    (input_df[\"dataset\"] == \"cagi4_sumo_ligase\")\n",
    "#     & (input_df[\"dataset\"] == \"cagi4_sumo_ligase\")\n",
    "    & (input_df[\"rev\"] == False)\n",
    "].dropna(subset=[\"effect\", \"provean_score\"])\n",
    "\n",
    "# df = input_df[(input_df[\"effect_type\"] == \"ΔΔG\") & (input_df[\"rev\"] == False)]\n",
    "\n",
    "for column in [\"provean_score\", \"foldx_score\", \"elaspic_score\", \"rosetta_dg_change\"]:\n",
    "    corr = stats.spearmanr(df[\"effect\"], df[column])\n",
    "    print(f\"{column:30s} {corr[0]:+.4} {corr[1]:.4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df[\"dataset\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_clusters(input_sequences, min_seq_id=0.3):\n",
    "    with tempfile.TemporaryDirectory() as tmp_dir:\n",
    "        input_dir = Path(tmp_dir, \"input\")\n",
    "        input_dir.mkdir()\n",
    "\n",
    "        output_dir = Path(tmp_dir, \"output\")\n",
    "        output_dir.mkdir()\n",
    "\n",
    "        scratch_dir = Path(tmp_dir, \"scratch\")\n",
    "        scratch_dir.mkdir()\n",
    "\n",
    "        with input_dir.joinpath(\"input.fasta\").open(\"wt\") as fout:\n",
    "            for tup in input_sequences.itertuples():\n",
    "                fout.write(f\">{tup.unique_id}\\n{tup.protein_sequence}\\n\")\n",
    "\n",
    "        system_command = f\"mmseqs easy-cluster --min-seq-id {min_seq_id} '{input_dir}/input.fasta' '{output_dir}/result' '{scratch_dir}'\"\n",
    "        print(system_command)\n",
    "\n",
    "        proc = subprocess.run(shlex.split(system_command), capture_output=True, check=True)\n",
    "\n",
    "        cluster_df = pd.read_csv(\n",
    "            output_dir.joinpath(\"result_cluster.tsv\"), sep=\"\\t\", names=[\"cluster_id\", \"unique_id\"]\n",
    "        )\n",
    "        assert len(cluster_df) == len(cluster_df[\"unique_id\"].unique())\n",
    "\n",
    "    return cluster_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequences = sequence_df.merge(input_df[[\"unique_id\"]].drop_duplicates())\n",
    "\n",
    "len(input_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_df = obtain_clusters(input_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"cluster_id\" in input_df:\n",
    "    del input_df[\"cluster_id\"]\n",
    "\n",
    "input_df = input_df.merge(cluster_df, on=\"unique_id\", how=\"outer\", validate=\"m:1\")\n",
    "assert input_df[\"cluster_id\"].notnull().all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "columns_to_delete = [\n",
    "    \"protbert_core_features_residue_wt\",\n",
    "    \"protbert_core_features_protein_wt\",\n",
    "    \"protbert_core_features_residue_mut\",\n",
    "    \"protbert_core_features_protein_mut\",\n",
    "]\n",
    "\n",
    "for column in columns_to_delete:\n",
    "    if column in input_df:\n",
    "        del input_df[column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "def map_to_test_fold(df):\n",
    "    @dataclass(order=True)\n",
    "    class PrioritizedItem:\n",
    "        priority: int\n",
    "        idx: int = field(compare=False)\n",
    "        data: Any = field(compare=False)\n",
    "\n",
    "    ddg_df = df[df[\"effect_type\"] == \"ΔΔG\"]\n",
    "    score_df = df[df[\"effect_type\"] == \"Deleteriousness score\"]\n",
    "    other_df = df[df[\"effect_type\"] == \"Deleteriousness class\"]\n",
    "    assert len(ddg_df) + len(score_df) + len(other_df) == len(df)\n",
    "\n",
    "    ddg_pq = [PrioritizedItem(0, i, []) for i in range(10)]\n",
    "    for cluster_id, gp in ddg_df.groupby(\"cluster_id\"):\n",
    "        item = heapq.heappop(ddg_pq)\n",
    "        item.priority += len(gp)\n",
    "        item.data.append(cluster_id)\n",
    "        heapq.heappush(ddg_pq, item)\n",
    "\n",
    "    mapping = {}\n",
    "    for item in ddg_pq:\n",
    "        for cluster_id in item.data:\n",
    "            mapping[cluster_id] = item.idx\n",
    "\n",
    "    del ddg_pq\n",
    "\n",
    "    score_pq = [PrioritizedItem(0, i, []) for i in range(10)]\n",
    "    for cluster_id, gp in score_df.groupby(\"cluster_id\"):\n",
    "        if cluster_id in mapping:\n",
    "            item_idx = mapping[cluster_id]\n",
    "            item = next(item for item in score_pq if item.idx == item_idx)\n",
    "            item.priority += len(gp)\n",
    "            item.data.append(cluster_id)\n",
    "            heapq.heapify(score_pq)\n",
    "        else:\n",
    "            item = heapq.heappop(score_pq)\n",
    "            item.priority += len(gp)\n",
    "            item.data.append(cluster_id)\n",
    "            heapq.heappush(score_pq, item)\n",
    "\n",
    "    for item in score_pq:\n",
    "        for cluster_id in item.data:\n",
    "            if cluster_id in mapping:\n",
    "                assert mapping[cluster_id] == item.idx\n",
    "            else:\n",
    "                mapping[cluster_id] = item.idx\n",
    "\n",
    "    del score_pq\n",
    "\n",
    "    other_pq = [PrioritizedItem(0, i, []) for i in range(10)]\n",
    "    for cluster_id, gp in other_df.groupby(\"cluster_id\"):\n",
    "        if cluster_id in mapping:\n",
    "            item_idx = mapping[cluster_id]\n",
    "            item = next(item for item in other_pq if item.idx == item_idx)\n",
    "            item.priority += len(gp)\n",
    "            item.data.append(cluster_id)\n",
    "            heapq.heapify(other_pq)\n",
    "        else:\n",
    "            item = heapq.heappop(other_pq)\n",
    "            item.priority += len(gp)\n",
    "            item.data.append(cluster_id)\n",
    "            heapq.heappush(other_pq, item)\n",
    "\n",
    "    for item in other_pq:\n",
    "        for cluster_id in item.data:\n",
    "            if cluster_id in mapping:\n",
    "                assert mapping[cluster_id] == item.idx\n",
    "            else:\n",
    "                mapping[cluster_id] = item.idx\n",
    "\n",
    "    return mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_id_to_test_fold_mapping = map_to_test_fold(input_df)\n",
    "input_df[\"test_fold\"] = input_df[\"cluster_id\"].map(cluster_id_to_test_fold_mapping)\n",
    "assert input_df[\"test_fold\"].notnull().all()\n",
    "assert len(input_df[\"test_fold\"].unique()) == 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(df):\n",
    "    effect = df[\"effect\"].values.copy()\n",
    "    \n",
    "    mask = df[\"effect_type\"] == \"Deleteriousness class\"\n",
    "    assert mask.any()\n",
    "    effect[mask] *= 3\n",
    "\n",
    "    mask = df[\"effect_type\"] == \"Deleteriousness score\"\n",
    "    if mask.any():\n",
    "        assert effect[mask].min() >= -5 and effect[mask].max() <= 5\n",
    "\n",
    "    effect = np.rint(np.clip(effect, -5, 5) * 1000 + 5000)\n",
    "    return effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df[\"label\"] = get_label(input_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_group(df):\n",
    "    vc = df[\"unique_id\"].value_counts()\n",
    "    groups = np.array([vc[uid] for uid in df[\"unique_id\"].unique()])\n",
    "    return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTEBOOK_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 10\n",
    "\n",
    "train_test_splits = []\n",
    "\n",
    "ps = PredefinedSplit(input_df[\"test_fold\"])\n",
    "for i, (train, test) in enumerate(tqdm(ps.split(), total=n_components)):\n",
    "    train_df = input_df.iloc[train].copy()\n",
    "    test_df = input_df.iloc[test].copy()\n",
    "    assert not set(train_df[\"cluster_id\"]) & set(test_df[\"cluster_id\"])\n",
    "\n",
    "    for column in pca_columns:\n",
    "        train_values = np.vstack(train_df[column].values)\n",
    "        test_values = np.vstack(test_df[column].values)\n",
    "\n",
    "        pickle_file = NOTEBOOK_DIR.joinpath(f\"{column}_pca{i}.pickle\")\n",
    "        if pickle_file.is_file():\n",
    "            pca = torch.load(pickle_file)\n",
    "        else:\n",
    "            pca = PCA(n_components=n_components)\n",
    "            pca.fit(train_values)\n",
    "            torch.save(pca, pickle_file)\n",
    "\n",
    "        train_values_out = pca.transform(train_values)\n",
    "        test_values_out = pca.transform(test_values)\n",
    "        for i in range(n_components):\n",
    "            new_column = f\"{column}_{i}_pca\"\n",
    "            train_df[new_column] = train_values_out[:, i]\n",
    "            test_df[new_column] = test_values_out[:, i]\n",
    "        del train_df[column], test_df[column]\n",
    "    train_test_splits.append((train_df, test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [\n",
    "    c\n",
    "    for c in list(train_test_splits[0][0])\n",
    "    if (c.endswith(\"_wt\") or c.endswith(\"_mut\") or c.endswith(\"_change\") or c.endswith(\"_pca\"))\n",
    "    and not (c.endswith(\"dg_change\") or c.startswith(\"rosetta_\"))\n",
    "]\n",
    "\n",
    "# feature_columns = [f for f in feature_columns if not f.startswith(\"rosetta_\")]\n",
    "feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_columns = [c for c in list(input_df) if c not in feature_columns]\n",
    "\n",
    "other_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate(train_test_splits, param):\n",
    "    result_dfs = []\n",
    "    for train_df, test_df in train_test_splits:\n",
    "        assert not set(train_df[\"cluster_id\"]) & set(test_df[\"cluster_id\"])\n",
    "\n",
    "        train_ds = lgb.Dataset(\n",
    "            train_df[feature_columns],\n",
    "            label=train_df[\"label\"],\n",
    "            group=get_group(train_df),\n",
    "        )\n",
    "\n",
    "        valid_ds = lgb.Dataset(\n",
    "            test_df[feature_columns],\n",
    "            label=test_df[\"label\"],\n",
    "            group=get_group(test_df),\n",
    "            reference=train_ds,\n",
    "        )\n",
    "\n",
    "        bst = lgb.train(\n",
    "            param, train_ds, num_boost_round=100, valid_sets=[valid_ds], verbose_eval=10000\n",
    "        )\n",
    "\n",
    "        test_df[\"ddg_pred\"] = bst.predict(\n",
    "            test_df[feature_columns], num_iteration=bst.best_iteration\n",
    "        )\n",
    "        result_dfs.append(test_df)\n",
    "    result_df = pd.concat(result_dfs, ignore_index=True)\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    param = {\n",
    "        \"objective\": \"lambdarank\",\n",
    "        \"metric\": \"ndcg\",\n",
    "        \"verbosity\": -1,\n",
    "        \"eval_at\": 1_000_000,\n",
    "        \"label_gain\": [(np.log2(i + 1) + 1) for i in range(0, 10_001)],\n",
    "        \"force_col_wise\": True,\n",
    "        #\n",
    "        \"num_boost_round\": 100,\n",
    "        \"lambda_l1\": trial.suggest_loguniform(\"lambda_l1\", 1e-8, 10.0),\n",
    "        \"lambda_l2\": trial.suggest_loguniform(\"lambda_l2\", 1e-8, 10.0),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n",
    "        \"feature_fraction\": trial.suggest_uniform(\"feature_fraction\", 0.4, 1.0),\n",
    "        \"bagging_fraction\": trial.suggest_uniform(\"bagging_fraction\", 0.4, 1.0),\n",
    "        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n",
    "    }\n",
    "    result_df = cross_validate(train_test_splits, param)\n",
    "    df = result_df[(result_df[\"dataset\"] == \"protherm++\") & (result_df[\"rev\"] == False)].dropna(\n",
    "        subset=[\"ddg_pred\", \"effect\"]\n",
    "    )\n",
    "    corr = stats.spearmanr(df[\"effect\"], df[\"ddg_pred\"])[0]\n",
    "    return corr\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    #\n",
    "    \"objective\": \"lambdarank\",\n",
    "    #     \"objective\": \"rank_xendcg\",\n",
    "    \"metric\": \"ndcg\",\n",
    "    \"eval_at\": 1_000_000,\n",
    "    \"label_gain\": [(np.log2(i + 1) + 1) for i in range(0, 10_001)],\n",
    "    \"force_col_wise\": True,\n",
    "    #\n",
    "    \"max_bin\": 255,\n",
    "    #     \"num_trees\": 100,  # aka num_boost_round\n",
    "    \"learning_rate\": 0.1,\n",
    "}\n",
    "\n",
    "result_df = cross_validate(train_test_splits, param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df[\"provean_score\"] = -result_df[\"provean_score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst.best_score  # 0.9979310001302695"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(input_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if COI == \"core\":\n",
    "    eval_columns = [\n",
    "        \"ddg_pred\",\n",
    "        \"provean_score\",\n",
    "        \"protbert_core_score_change\",\n",
    "        \"proteinsolver_core_score_change\",\n",
    "        \"foldx_score\",\n",
    "        \"elaspic_score\",\n",
    "        \"rosetta_dg_change\",\n",
    "    ]\n",
    "else:\n",
    "    eval_columns = [\n",
    "        \"ddg_pred\",\n",
    "        \"protbert_core_score_change\",\n",
    "        \"proteinsolver_core_score_change\",\n",
    "        \"provean_score\",\n",
    "        \"foldx_score\",\n",
    "        \"elaspic_score\",\n",
    "        \"rosetta_opt_apart_dg_change\",\n",
    "        \"rosetta_apart_dg_change\",\n",
    "        \"rosetta_complex_dg_change\",\n",
    "        \"rosetta_opt_bind_dg_change\",\n",
    "        \"rosetta_bind_dg_change\",\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spearman_corrs_global(df, feature_columns, target_column):\n",
    "    corrs = {}\n",
    "    for column in feature_columns:\n",
    "        sub_df = df.dropna(subset=[column, target_column])\n",
    "        corr = stats.spearmanr(sub_df[column], sub_df[target_column])\n",
    "        corrs[column] = (corr[0], corr[1], len(sub_df))\n",
    "        # print(f\"{column:30s} {corr[0]:+.4} {corr[1]:.4}\")\n",
    "    return corrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spearman_corrs_perseq(df, feature_columns, target_column, min_gp_size=6):\n",
    "    df = df.dropna(subset=feature_columns + [target_column])\n",
    "    results = {c: [] for c in feature_columns}\n",
    "    for _, gp in df.groupby(\"unique_id\"):\n",
    "        if len(gp) < min_gp_size or len(set(gp[target_column])) < 2:\n",
    "            continue\n",
    "        for column in feature_columns:\n",
    "            corr = stats.spearmanr(gp[column], gp[target_column])\n",
    "            results[column].append(corr[0])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_spearman_corrs(corrs):\n",
    "    for column, corr in corrs.items():\n",
    "        print(f\"{column:30s} {corr[0]:+.4} {corr[1]:.4} ({corr[2]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_spearman_corrs(get_spearman_corrs_global(result_df, eval_columns, \"effect\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_spearman_corrs(\n",
    "    get_spearman_corrs_global(\n",
    "        result_df[\n",
    "            #\n",
    "            (result_df[\"dataset\"] == \"protherm++\")\n",
    "            & (result_df[\"rev\"] == False)\n",
    "        ],\n",
    "        eval_columns,\n",
    "        \"effect\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_spearman_corrs(\n",
    "    get_spearman_corrs_global(\n",
    "        result_df[\n",
    "            #\n",
    "            (result_df[\"dataset\"] == \"taipale_gpca\")\n",
    "            & (result_df[\"rev\"] == False)\n",
    "        ],\n",
    "        eval_columns,\n",
    "        \"effect\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df[\"dataset\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_spearman_corrs(\n",
    "    get_spearman_corrs_global(result_df[result_df[\"effect_type\"] == \"Deleteriousness score\"], eval_columns, \"effect\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df[\"dataset\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_spearman_corrs(\n",
    "    get_spearman_corrs_global(\n",
    "        result_df[\n",
    "            #\n",
    "            (result_df[\"dataset\"] == \"taipale_ppi\")\n",
    "            & (result_df[\"rev\"] == False)\n",
    "        ],\n",
    "        eval_columns,\n",
    "        \"effect\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skempiskempi, ab_bind, taipale_ppi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_spearman_corrs(\n",
    "    get_spearman_corrs_global(result_df[result_df[\"effect_type\"] == \"ΔΔG\"], eval_columns, \"effect\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df[result_df[\"effect_type\"] == \"ΔΔG\"][\"dataset\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import set_matplotlib_formats\n",
    "\n",
    "set_matplotlib_formats(\"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIGURE_OUTPUT_DIR = Path(f\"05_model_validation_{COI}\").resolve()\n",
    "FIGURE_OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "FIGURE_OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = plt.cm.get_cmap(\"tab20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df[\"dataset\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"humsavar\"\n",
    "rev = [False, True]\n",
    "\n",
    "effect_type = {\"skempi++\": \"ΔΔG\", \"cagi4_sumo_ligase\": \"Deleteriousness score\"}.get(\n",
    "    dataset, \"Deleteriousness class\"\n",
    ")\n",
    "\n",
    "suffix = f\"-{dataset}\"\n",
    "if rev != [False, True]:\n",
    "    assert rev == [False]\n",
    "    suffix += \"-norev\"\n",
    "\n",
    "\n",
    "df = result_df[\n",
    "    (result_df[\"effect_type\"] == effect_type)\n",
    "    & (result_df[\"dataset\"] == dataset)\n",
    "    & (result_df[\"rev\"].isin(rev))\n",
    "]\n",
    "\n",
    "\n",
    "corrs = get_spearman_corrs_global(df, eval_columns, \"effect\")\n",
    "fg, ax = plt.subplots(figsize=(6, 6))\n",
    "x = np.arange(len(corrs))\n",
    "y = [c[0] for c in corrs.values()]\n",
    "out = ax.bar(x, y, color=cmap(1), edgecolor=\"k\")\n",
    "_ = ax.set_xticks(x)\n",
    "_ = ax.set_xticklabels(corrs.keys(), rotation=\"vertical\")\n",
    "ax.set_ylabel(\"Spearman's ρ\")\n",
    "ax.set_title(\"Global correlations\")\n",
    "fg.subplots_adjust(top=0.95, right=0.98, bottom=0.38)\n",
    "fg.savefig(FIGURE_OUTPUT_DIR.joinpath(f\"corrs-global{suffix}.svg\"), dpi=300)\n",
    "fg.savefig(FIGURE_OUTPUT_DIR.joinpath(f\"corrs-global{suffix}.png\"), dpi=300)\n",
    "fg.savefig(FIGURE_OUTPUT_DIR.joinpath(f\"corrs-global{suffix}.pdf\"), dpi=300)\n",
    "\n",
    "\n",
    "per_sequence_stats = get_spearman_corrs_perseq(df, eval_columns, \"effect\", min_gp_size=6)\n",
    "fg, ax = plt.subplots(figsize=(6, 6))\n",
    "out = ax.boxplot(\n",
    "    per_sequence_stats.values(),\n",
    "    patch_artist=True,\n",
    "    boxprops={\"facecolor\": cmap(1)},\n",
    "    medianprops={\"color\": cmap(0)},\n",
    ")\n",
    "bp = ax.set_xticklabels(per_sequence_stats.keys(), rotation=\"vertical\")\n",
    "ax.set_ylabel(\"Spearman's ρ\")\n",
    "ax.set_title(\"Per-protein correlations\")\n",
    "fg.subplots_adjust(top=0.95, right=0.98, bottom=0.38)\n",
    "fg.savefig(FIGURE_OUTPUT_DIR.joinpath(f\"corrs-perseq{suffix}.svg\"), dpi=300)\n",
    "fg.savefig(FIGURE_OUTPUT_DIR.joinpath(f\"corrs-perseq{suffix}.png\"), dpi=300)\n",
    "fg.savefig(FIGURE_OUTPUT_DIR.joinpath(f\"corrs-perseq{suffix}.pdf\"), dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df[\"rev\"] == True).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "\n",
    "SVG(FIGURE_OUTPUT_DIR.joinpath(f\"corrs-global{suffix}.svg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls {FIGURE_OUTPUT_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = result_df[\n",
    "    (result_df[\"effect_type\"] == \"ΔΔG\")\n",
    "    & (result_df[\"dataset\"] == \"skempi++\")\n",
    "    & (result_df[\"rev\"].isin([False]))\n",
    "]\n",
    "\n",
    "corrs = get_spearman_corrs_global(df, eval_columns, \"effect\")\n",
    "fg, ax = plt.subplots()\n",
    "x = np.arange(len(corrs))\n",
    "y = [c[0] for c in corrs.values()]\n",
    "out = ax.bar(x, y, color=cmap(1), edgecolor=\"k\")\n",
    "_ = ax.set_xticks(x)\n",
    "_ = ax.set_xticklabels(corrs.keys(), rotation=\"vertical\")\n",
    "ax.set_ylabel(\"Spearman's ρ\")\n",
    "ax.set_title(\"Global correlations\")\n",
    "fg.savefig(FIGURE_OUTPUT_DIR.joinpath(\"corrs-global-skempi-norev.svg\"), dpi=300)\n",
    "fg.savefig(FIGURE_OUTPUT_DIR.joinpath(\"corrs-global-skempi.png\"), dpi=300)\n",
    "fg.savefig(FIGURE_OUTPUT_DIR.joinpath(\"corrs-global-skempi.pdf\"), dpi=300)\n",
    "\n",
    "per_sequence_stats = get_spearman_corrs_perseq(result_df, eval_columns, \"effect\", min_gp_size=6)\n",
    "fg, ax = plt.subplots()\n",
    "out = ax.boxplot(\n",
    "    per_sequence_stats.values(),\n",
    "    patch_artist=True,\n",
    "    boxprops={\"facecolor\": cmap(1)},\n",
    "    medianprops={\"color\": cmap(0)},\n",
    ")\n",
    "bp = ax.set_xticklabels(per_sequence_stats.keys(), rotation=\"vertical\")\n",
    "ax.set_ylabel(\"Spearman's ρ\")\n",
    "ax.set_title(\"Per-protein correlations\")\n",
    "fg.savefig(FIGURE_OUTPUT_DIR.joinpath(\"corrs-perseq-skempi.svg\"), dpi=300)\n",
    "fg.savefig(FIGURE_OUTPUT_DIR.joinpath(\"corrs-perseq-skempi.png\"), dpi=300)\n",
    "fg.savefig(FIGURE_OUTPUT_DIR.joinpath(\"corrs-perseq-skempi.pdf\"), dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_spearman_stats(\n",
    "    result_df[\n",
    "        (result_df[\"effect_type\"] == \"Deleteriousness class\")\n",
    "        & (result_df[\"rev\"].isin([True, False]))\n",
    "    ],\n",
    "    eval_columns,\n",
    "    \"effect\",\n",
    ")\n",
    "# 0.488"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df[\n",
    "    (result_df[\"effect_type\"] == \"Deleteriousness class\") & (result_df[\"rev\"].isin([True, False]))\n",
    "][\"dataset\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_spearman_stats(\n",
    "    result_df[\n",
    "        (result_df[\"effect_type\"] == \"Deleteriousness score\")\n",
    "        & (result_df[\"rev\"].isin([True, False]))\n",
    "    ],\n",
    "    eval_columns,\n",
    "    \"effect\",\n",
    ")\n",
    "# 0.4128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_spearman_stats(result_df, [\"ddg_pred\", \"rosetta_dg_change\"], \"label\")  # 0.4646"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_spearman_stats(result_df[result_df[\"effect_type\"] == \"Deleteriousness score\"], eval_columns, \"label\")  # 0.4077"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_spearman_stats(result_df[result_df[\"effect_type\"] == \"ΔΔG\"], eval_columns, \"effect\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_per_sequence_stats(df, feature_columns, target_column, min_gp_size=6):\n",
    "    df = df.dropna(subset=feature_columns + [target_column])\n",
    "    results = {c: [] for c in feature_columns}\n",
    "    for _, gp in df.groupby(\"unique_id\"):\n",
    "        if len(gp) < min_gp_size or len(set(gp[target_column])) < 2:\n",
    "            continue\n",
    "        for column in feature_columns:\n",
    "            corr = stats.spearmanr(gp[column], gp[target_column])\n",
    "            results[column].append(corr[0])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_sequence_stats = compute_per_sequence_stats(result_df, eval_columns, \"effect\", 6)\n",
    "\n",
    "fg, ax = plt.subplots()\n",
    "\n",
    "out = ax.boxplot(per_sequence_stats.values())\n",
    "_ = ax.set_xticklabels(per_sequence_stats.keys(), rotation=\"vertical\")\n",
    "# ax.set_ylim(-1, 1)\n",
    "# fg.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_sequence_stats_ddg = compute_per_sequence_stats(\n",
    "    result_df[result_df[\"effect_type\"] == \"Deleteriousness class\"], eval_columns, \"effect\", 18\n",
    ")\n",
    "\n",
    "fg, ax = plt.subplots()\n",
    "\n",
    "out = ax.boxplot(per_sequence_stats_ddg.values())\n",
    "_ = ax.set_xticklabels(per_sequence_stats_ddg.keys(), rotation=\"vertical\")\n",
    "# ax.set_ylim(-1, 1)\n",
    "# fg.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_sequence_stats_ddg = compute_per_sequence_stats(\n",
    "    result_df[result_df[\"effect_type\"] == \"Deleteriousness score\"], eval_columns, \"effect\", 18\n",
    ")\n",
    "\n",
    "fg, ax = plt.subplots()\n",
    "\n",
    "out = ax.boxplot(per_sequence_stats_ddg.values())\n",
    "_ = ax.set_xticklabels(per_sequence_stats_ddg.keys(), rotation=\"vertical\")\n",
    "# ax.set_ylim(-1, 1)\n",
    "# fg.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = [\"r\", \"g\", \"b\", \"y\"]\n",
    "for x, val, c in zip(xs, vals, palette):\n",
    "    plt.scatter(x, val, alpha=0.4, color=c)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[(train_df[\"effect\"] * 1_000).astype(np.int) > 300_000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "_ = plt.hist(input_df[\"effect\"], bins=100, range=(-5, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    \"objective\": \"lambdarank\",\n",
    "    \"metric\": \"ndcg\",\n",
    "    \"ndcg_eval_at\": 1000000000000,\n",
    "    \"max_bin\": 255,\n",
    "}\n",
    "\n",
    "\n",
    "bst = lgb.train(param, train_ds, num_boost_round=100, valid_sets=[valid_ds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = bst.predict(test_df.drop(columns_to_drop, axis=1), num_iteration=bst.best_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = bst.predict(test_df.drop(columns_to_drop, axis=1), num_iteration=bst.best_iteration)\n",
    "test_df = test_df.copy()\n",
    "test_df[\"ddg_pred\"] = ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.spearmanr(test_df[\"effect\"], test_df[\"ddg_pred\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.spearmanr(test_df[\"effect\"], test_df[\"foldx_score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.spearmanr(test_df[\"effect\"], test_df[\"provean_score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
