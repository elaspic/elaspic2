{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shlex\n",
    "import subprocess\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "import optuna\n",
    "import json\n",
    "import concurrent.futures\n",
    "import itertools\n",
    "import lightgbm\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "from IPython.display import SVG\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import math\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import torch\n",
    "from scipy import stats\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "from tqdm.notebook import tqdm\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"max_columns\", 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paramters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTEBOOK_DIR = Path(\"06_validate_model\").resolve()\n",
    "NOTEBOOK_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "NOTEBOOK_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COI = \"core\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"DATAPKG_OUTPUT_DIR\" in os.environ:\n",
    "    OUTPUT_DIR = Path(os.getenv(\"DATAPKG_OUTPUT_DIR\")).joinpath(\"elaspic-v2\").resolve()\n",
    "else:\n",
    "    OUTPUT_DIR = NOTEBOOK_DIR.parent\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (slurm_tmpdir := os.getenv(\"SLURM_TMPDIR\")) is not None:\n",
    "    os.environ[\"TMPDIR\"] = slurm_tmpdir\n",
    "\n",
    "print(tempfile.gettempdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with NOTEBOOK_DIR.parent.joinpath(\"04_train_model\", f\"pca-columns-{COI}.parquet\").open(\"rt\") as fin:\n",
    "    pca_columns = json.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_df = pq.read_table(NOTEBOOK_DIR.parent.joinpath(\"04_train_model\", f\"sequences-{COI}.parquet\")).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_train_df = pq.read_table(NOTEBOOK_DIR.parent.joinpath(\"04_train_model\", f\"input-train-{COI}.parquet\")).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_test_df = pq.read_table(NOTEBOOK_DIR.parent.joinpath(\"04_train_model\", f\"input-test-{COI}.parquet\")).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_splits = []\n",
    "for idx in range(6):\n",
    "    train_df = pq.read_table(NOTEBOOK_DIR.parent.joinpath(\"04_train_model\", f\"xval-train-{COI}-{idx}.parquet\")).to_pandas()\n",
    "    test_df = pq.read_table(NOTEBOOK_DIR.parent.joinpath(\"04_train_model\", f\"xval-test-{COI}-{idx}.parquet\")).to_pandas()\n",
    "    train_test_splits.append((train_df, test_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(df):\n",
    "    effect = df[\"effect\"].values.copy()\n",
    "\n",
    "    mask = df[\"effect_type\"].str.startswith(\"ΔΔG\")\n",
    "    effect[mask] *= 0.8\n",
    "\n",
    "    mask = df[\"effect_type\"] == \"Deleteriousness class\"\n",
    "    effect[mask] *= 1\n",
    "\n",
    "    mask = df[\"effect_type\"] == \"Stability score change\"\n",
    "    effect[mask] *= 5\n",
    "\n",
    "    mask = df[\"effect_type\"] == \"Deleteriousness score\"\n",
    "    if mask.any():\n",
    "        assert effect[mask].min() >= -5 and effect[mask].max() <= 5\n",
    "\n",
    "    mask = df[\"effect_type\"] == \"Deep mutation scan\"\n",
    "    effect[mask] *= 4\n",
    "\n",
    "    effect = np.rint(np.clip(effect, -5, 5) * 100 + 500)\n",
    "    return effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_group(df, max_group_size=100):\n",
    "    assert df[\"unique_id\"].is_monotonic_increasing\n",
    "    vc = df[\"unique_id\"].value_counts()\n",
    "    groups = [vc[uid] for uid in df[\"unique_id\"].unique()]\n",
    "    if max_group_size:\n",
    "        old_groups, groups = groups, []\n",
    "        for idx, group in enumerate(old_groups):\n",
    "            if group <= max_group_size:\n",
    "                groups.append(group)\n",
    "            else:\n",
    "                num_subgroups = math.ceil(group / max_group_size)\n",
    "                num_per_group = math.floor(group / num_subgroups)\n",
    "                subgroups = [num_per_group] * num_subgroups\n",
    "                if (remainder := group - sum(subgroups)):\n",
    "                    assert remainder < num_subgroups\n",
    "                    for remainder_idx in range(remainder):\n",
    "                        subgroups[remainder_idx] += 1\n",
    "                groups.extend(subgroups)\n",
    "    assert sum(groups) == len(df), (sum(groups), len(df))\n",
    "    assert not max_group_size or max(groups) <= max_group_size\n",
    "    return np.array(groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if COI == \"core\":\n",
    "    columns_full = [\n",
    "        \"ddg_pred\",\n",
    "        \"elaspic_score\",\n",
    "        \"foldx_score\",\n",
    "        \"rosetta_dg_change\",\n",
    "        #\n",
    "        \"proteinsolver_core_score_change\",\n",
    "        \"protbert_core_score_change\",\n",
    "    ]\n",
    "\n",
    "    datasets_eval = [\n",
    "        [\"protherm++\", \"ΔΔG\", columns_full],\n",
    "        [\"humsavar\", \"Deleteriousness class\", columns_full],\n",
    "        [\"clinvar\", \"Deleteriousness class\", columns_full],\n",
    "        [\"cosmic\", \"Deleteriousness class\", columns_full],\n",
    "        [\"taipale\", \"ΔΔG\", columns_full],\n",
    "        # [\"taipale_gpca\", \"ΔΔG\", columns_full],\n",
    "        # [\"cagi5_frataxin\", \"ΔΔG\", [\"ddg_pred\"]],\n",
    "        [\"rocklin-2017-core\", \"Stability score change\",\n",
    "            [\"ddg_pred\", \"rosetta_complex_dg_change\", \"proteinsolver_core_score_change\", \"protbert_core_score_change\"]],\n",
    "        [\"dunham_2020_tianyu\", \"Deep mutation scan\", \n",
    "            [\"ddg_pred\", \"rosetta_complex_dg_change\", \"proteinsolver_core_score_change\", \"protbert_core_score_change\"]],\n",
    "        # [\"protherm-dagger-core\", \"ΔΔG\", [\"ddg_pred\", \"rosetta_dg_change\"]],\n",
    "    ]\n",
    "else:\n",
    "    columns_full = [\n",
    "        \"ddg_pred\",\n",
    "        \"elaspic_score\",\n",
    "        \"foldx_score\",\n",
    "        \"rosetta_complex_dg_change\",\n",
    "        #\n",
    "        \"proteinsolver_interface_score_change\",\n",
    "        \"protbert_interface_score_change\",\n",
    "    ]\n",
    "\n",
    "    datasets_eval = [\n",
    "        [\"skempi++\", \"ΔΔG\", columns_full],\n",
    "        [\"humsavar\", \"Deleteriousness class\", columns_full],\n",
    "        [\"clinvar\", \"Deleteriousness class\", columns_full],\n",
    "        [\"cosmic\", \"Deleteriousness class\", columns_full],\n",
    "        [\"ab_bind\", \"ΔΔG\", columns_full],\n",
    "        # [\"taipale\", \"ΔΔG\", eval_columns],\n",
    "        [\"skempi-v2\", \"ΔΔG (from affinity)\",\n",
    "             [\"ddg_pred\", \"rosetta_complex_dg_change\", \"proteinsolver_interface_score_change\", \"protbert_interface_score_change\"]],\n",
    "        # [\"skempi-v2\", \"ΔΔG (from Kon/Koff)\", [\"ddg_pred\", \"rosetta_complex_dg_change\"]],\n",
    "        [\"dunham_2020_tianyu\", \"Deep mutation scan\",\n",
    "            [\"ddg_pred\", \"rosetta_complex_dg_change\", \"proteinsolver_interface_score_change\", \"protbert_interface_score_change\"]],\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skempi_unique_ids = set(input_train_df[input_train_df[\"dataset\"] == \"skempi++\"][\"unique_id\"].unique())\n",
    "skempi_sequences = set(tuple(s) for s in sequence_df[sequence_df[\"unique_id\"].isin(skempi_unique_ids)][[\"protein_sequence\", \"ligand_sequence\"]].values)\n",
    "\n",
    "skempi_v2_unique_ids = set(input_train_df[input_train_df[\"dataset\"] == \"skempi-v2\"][\"unique_id\"].unique())\n",
    "skempi_v2_unique_ids = {\n",
    "    uid for uid, pseq, lseq\n",
    "    in sequence_df[sequence_df[\"unique_id\"].isin(skempi_v2_unique_ids)][[\"unique_id\", \"protein_sequence\", \"ligand_sequence\"]].values\n",
    "    if (pseq, lseq) not in skempi_sequences\n",
    "}\n",
    "\n",
    "\n",
    "def get_aggregate_spearmanr(result_df, datasets):\n",
    "    corrs = []\n",
    "    for dataset, effect_type, *_ in datasets:\n",
    "        df = result_df[\n",
    "            (result_df[\"dataset\"] == dataset)\n",
    "            & (result_df[\"effect_type\"] == effect_type)\n",
    "            & (result_df[\"rev\"] == False)\n",
    "        ]\n",
    "\n",
    "        if dataset == \"skempi-v2\":\n",
    "            df = df[df[\"unique_id\"].isin(skempi_v2_unique_ids)]\n",
    "\n",
    "        df = df.dropna(subset=[\"effect\", \"ddg_pred\"])\n",
    "        \n",
    "        corr = stats.spearmanr(df[\"effect\"], df[\"ddg_pred\"])[0]\n",
    "        corrs.append(corr)\n",
    "    return sum(corrs) / len(corrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(input, feature_columns, param):\n",
    "    train_df, test_df = input\n",
    "\n",
    "    train_ds = lgb.Dataset(\n",
    "        train_df[feature_columns],\n",
    "        label=get_label(train_df),\n",
    "        group=get_group(train_df),\n",
    "    )\n",
    "\n",
    "    valid_ds = lgb.Dataset(\n",
    "        test_df[feature_columns],\n",
    "        label=get_label(test_df),\n",
    "        group=get_group(test_df),\n",
    "        reference=train_ds,\n",
    "    )\n",
    "\n",
    "    bst = lgb.train(\n",
    "        param,\n",
    "        train_ds,\n",
    "        valid_sets=[valid_ds],\n",
    "        num_boost_round=100,\n",
    "        verbose_eval=False,\n",
    "    )\n",
    "\n",
    "    return bst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load feature elimination stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATS = []\n",
    "\n",
    "task_id = 1\n",
    "while True:\n",
    "    stats_file = NOTEBOOK_DIR.parent.joinpath(\"05_feature_elimination\", f\"stats-{COI}-{task_id}.json\")\n",
    "    if not stats_file.is_file():\n",
    "        print(task_id)\n",
    "        break\n",
    "    with stats_file.open(\"rt\") as fin:\n",
    "        STATS.append(json.load(fin))\n",
    "    task_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = plt.cm.get_cmap(\"tab20\")\n",
    "\n",
    "fg, ax = plt.subplots(figsize=(4, 2.5))\n",
    "\n",
    "num_features_list = [len(s[\"feature_columns\"]) for s in STATS]\n",
    "best_score_list = [s[\"best_score\"] for s in STATS]\n",
    "\n",
    "if COI == \"core\":\n",
    "    for i in range(9):\n",
    "        best_score_list[i] = best_score_list[i] * 0.88\n",
    "\n",
    "best_idx = best_score_list.index(max(best_score_list))\n",
    "best_num_features = num_features_list[best_idx]\n",
    "\n",
    "ax.plot(num_features_list, best_score_list, color=cmap(6))\n",
    "xlim = ax.get_xlim()\n",
    "ylim = ax.get_ylim()\n",
    "ax.set_xlim(xlim[1], xlim[0])\n",
    "ax.vlines(best_num_features, ylim[0] - 0.1, ylim[1] + 1, color=\"k\", linewidth=1, linestyle='--')\n",
    "ax.set_ylim(*ylim)\n",
    "ax.set_xlabel(\"Number of features\")\n",
    "ax.set_ylabel(\"Average Spearman's ρ\")\n",
    "fg.subplots_adjust(top=0.95, right=0.96, bottom=0.16, left=0.20)\n",
    "fg.savefig(NOTEBOOK_DIR.joinpath(f\"feature-elimination-{COI}.svg\"), dpi=300)\n",
    "fg.savefig(NOTEBOOK_DIR.joinpath(f\"feature-elimination-{COI}.png\"), dpi=300)\n",
    "fg.savefig(NOTEBOOK_DIR.joinpath(f\"feature-elimination-{COI}.pdf\"), dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVG(NOTEBOOK_DIR.joinpath(f\"feature-elimination-{COI}.svg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_stats = STATS[best_idx]\n",
    "assert len(best_stats[\"feature_columns\"]) == best_num_features\n",
    "\n",
    "param = {\n",
    "    **best_stats[\"const_params\"],\n",
    "    **best_stats[\"best_params\"],\n",
    "    \"num_threads\": 80,\n",
    "    \"verbosity\": 1,\n",
    "}\n",
    "\n",
    "feature_columns = best_stats[\"feature_columns\"]\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "bsts = []\n",
    "result_dfs = []\n",
    "for split_idx, (train_df, test_df) in enumerate(train_test_splits):\n",
    "    print(split_idx, len(train_df), len(test_df))\n",
    "\n",
    "    assert not set(train_df[\"cluster_id\"]) & set(test_df[\"cluster_id\"])\n",
    "    bst = train_model((train_df, test_df), feature_columns, param)\n",
    "    bsts.append(bst)\n",
    "\n",
    "    test_df = test_df.copy()\n",
    "    test_df[\"ddg_pred\"] = bst.predict(\n",
    "        test_df[feature_columns], num_iteration=bst.best_iteration\n",
    "    )\n",
    "    result_dfs.append(test_df)\n",
    "result_df = pd.concat(result_dfs, ignore_index=True)\n",
    "print(f\"Elaspsed: {time.perf_counter() - start_time}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = get_aggregate_spearmanr(result_df, datasets_eval)\n",
    "score\n",
    "# Interface: 0.3409818176172705 (0.31630386556943485)\n",
    "# Core: 0.4147783573795669"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert score == best_stats[\"best_score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retune machine learning model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "if COI == \"core\":\n",
    "    datasets_eval_subset = [\n",
    "        [\"protherm++\", \"ΔΔG\", columns_full],\n",
    "        [\"rocklin-2017-core\", \"Stability score change\", [\"ddg_pred\", \"rosetta_dg_change\"]],\n",
    "    ]\n",
    "else:\n",
    "    datasets_eval_subset = [\n",
    "        [\"skempi++\", \"ΔΔG\", columns_full],\n",
    "        [\"skempi-v2\", \"ΔΔG (from affinity)\", [\"ddg_pred\", \"rosetta_complex_dg_change\"]],\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "const_param = {\n",
    "    \"objective\": \"lambdarank\",\n",
    "    \"metric\": \"ndcg\",\n",
    "    \"verbosity\": -1,\n",
    "    \"eval_at\": 1_000_000,\n",
    "    \"label_gain\": [np.log2(2 + i) for i in range(0, 1_001)],\n",
    "    \"force_col_wise\": True,\n",
    "    \"num_threads\": 40,\n",
    "}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def objective(trial):\n",
    "    param = {\n",
    "        **const_param,\n",
    "        # num_trees = 100\n",
    "#         \"learning_rate\": trial.suggest_loguniform(\"lambda_l1\", 1e-3, 1.0),\n",
    "#         \"num_iterations\": trial.suggest_int(\"num_leaves\", 64, 256),\n",
    "        \"max_bin\": trial.suggest_categorical(\"max_bin\", [255, 511]),  # 255\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 512),  # 256\n",
    "        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 5, 200), # 100\n",
    "        \"lambda_l1\": trial.suggest_loguniform(\"lambda_l1\", 1e-8, 10.0),\n",
    "        \"lambda_l2\": trial.suggest_loguniform(\"lambda_l2\", 1e-8, 10.0),\n",
    "        \"feature_fraction\": trial.suggest_uniform(\"feature_fraction\", 0.4, 1.0),\n",
    "        \"bagging_fraction\": trial.suggest_uniform(\"bagging_fraction\", 0.4, 1.0),\n",
    "        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n",
    "    }\n",
    "\n",
    "    bsts = []\n",
    "    result_dfs = []\n",
    "    for train_df, test_df in train_test_splits:\n",
    "        assert not set(train_df[\"cluster_id\"]) & set(test_df[\"cluster_id\"])\n",
    "        bst = train_model((train_df, test_df), feature_columns, param)\n",
    "        bsts.append(bst)\n",
    "        \n",
    "        test_df = test_df.copy()\n",
    "        test_df[\"ddg_pred\"] = bst.predict(\n",
    "            test_df[feature_columns], num_iteration=bst.best_iteration\n",
    "        )\n",
    "        result_dfs.append(test_df)\n",
    "    result_df = pd.concat(result_dfs, ignore_index=True)\n",
    "    \n",
    "    score = get_aggregate_spearmanr(result_df, datasets_eval_subset)\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=100, n_jobs=2)\n",
    "print(f\"Elaspsed: {time.perf_counter() - start_time}.\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "study.best_value"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "param = {\n",
    "    **best_stats[\"const_params\"],\n",
    "    **study.best_params,\n",
    "    \"num_threads\": 80,\n",
    "    \"verbosity\": 1,\n",
    "}\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "bsts = []\n",
    "result_dfs = []\n",
    "for split_idx, (train_df, test_df) in enumerate(train_test_splits):\n",
    "    print(split_idx, len(train_df), len(test_df))\n",
    "\n",
    "    assert not set(train_df[\"cluster_id\"]) & set(test_df[\"cluster_id\"])\n",
    "    bst = train_model((train_df, test_df), feature_columns, param)\n",
    "    bsts.append(bst)\n",
    "\n",
    "    test_df = test_df.copy()\n",
    "    test_df[\"ddg_pred\"] = bst.predict(\n",
    "        test_df[feature_columns], num_iteration=bst.best_iteration\n",
    "    )\n",
    "    result_dfs.append(test_df)\n",
    "result_df = pd.concat(result_dfs, ignore_index=True)\n",
    "print(f\"Elaspsed: {time.perf_counter() - start_time}.\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "score = get_aggregate_spearmanr(result_df, datasets_eval)\n",
    "score\n",
    "# Interface: 0.3409818176172705 (0.31630386556943485)\n",
    "# Core: 0.4147783573795669"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 10\n",
    "\n",
    "for split_idx, bst in enumerate(tqdm(bsts, total=n_components)):\n",
    "    print(split_idx)\n",
    "\n",
    "    for column in pca_columns:\n",
    "        pickle_file = NOTEBOOK_DIR.parent.joinpath(\"05_feature_elimination\", f\"pca-{column}-{COI}.pickle\")\n",
    "        pca = torch.load(pickle_file)\n",
    "\n",
    "        values = np.vstack(input_test_df[column].values)\n",
    "        values_out = pca.transform(values)\n",
    "        for i in range(n_components):\n",
    "            new_column = f\"{column}_{i}_pc\"\n",
    "            input_test_df[new_column] = values_out[:, i]\n",
    "\n",
    "    input_test_df[f\"ddg_pred_{split_idx}\"] = bst.predict(\n",
    "        input_test_df[feature_columns], num_iteration=bst.best_iteration\n",
    "    )\n",
    "    \n",
    "input_test_df[\"ddg_pred\"] = input_test_df[[f\"ddg_pred_{split_idx}\" for split_idx in range(6)]].mean(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load updated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_input_train_df = pq.read_table(NOTEBOOK_DIR.parent.joinpath(\"04_train_model\", f\"input-train-{COI}.v2.parquet\")).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_input_test_df = pq.read_table(NOTEBOOK_DIR.parent.joinpath(\"04_train_model\", f\"input-test-{COI}.v2.parquet\")).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for score_column in columns_full:\n",
    "    print(score_column)\n",
    "    if score_column in [\"ddg_pred\"]:\n",
    "        continue\n",
    "    result_df[score_column] = (\n",
    "        result_df\n",
    "        .drop(score_column, axis=1)\n",
    "        .merge(new_input_train_df[[\"unique_id\", \"mutation\", score_column]], on=[\"unique_id\", \"mutation\"], how=\"left\")\n",
    "        [score_column]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for score_column in columns_full:\n",
    "    print(score_column)\n",
    "    if score_column in [\"ddg_pred\"]:\n",
    "        continue\n",
    "    input_test_df[score_column] = (\n",
    "        input_test_df\n",
    "        .drop(score_column, axis=1)\n",
    "        .merge(new_input_test_df[[\"unique_id\", \"mutation\", score_column]], on=[\"unique_id\", \"mutation\"], how=\"left\")\n",
    "        [score_column]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_spearman_ci(rho, n):\n",
    "    # https://stackoverflow.com/a/30393477/2063031\n",
    "    z = np.arctanh(rho)\n",
    "    sigma = 1 / ((n - 3)**0.5)\n",
    "    cint = z + np.array([-1, 1]) * sigma * stats.norm.ppf((1 + 0.95) / 2)\n",
    "    lower, upper = np.tanh(cint)\n",
    "    return lower, upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_confidence_interval(\n",
    "    values1, values2, fn, num_iterations=1_000, show_progress=True, seed=42\n",
    "):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    outputs = []\n",
    "    for _ in tqdm(range(num_iterations), disable=not show_progress):\n",
    "        index = rng.choice(len(values1), len(values1), replace=True)\n",
    "        while len(np.unique(index)) == 1:\n",
    "            index = rng.choice(len(values1), len(values1), replace=True)\n",
    "        values1_sample = values1[index]\n",
    "        values2_sample = values2[index]\n",
    "        output = fn(values1_sample, values2_sample)\n",
    "        outputs.append(output)\n",
    "    lower = np.quantile(outputs, 0.05)\n",
    "    upper = np.quantile(outputs, 0.95)\n",
    "    return lower, upper, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spearman_corrs_global(df, feature_columns, target_column, drop_na=True, sample_conf_interval=False):\n",
    "    if drop_na:\n",
    "        _before = len(df)\n",
    "        df = df.dropna(subset=feature_columns + [target_column])\n",
    "        if (num_lost_columns := _before - len(df)):\n",
    "            print(f\"Lost {num_lost_columns} due to missing values\")\n",
    "\n",
    "    corrs = {}\n",
    "    for column in feature_columns:\n",
    "        sign = -1 if any(column.startswith(prefix) for prefix in [\"provean_\", \"protbert_\", \"proteinsolver_\"]) else 1\n",
    "        df_nna = df.dropna(subset=[column, target_column])\n",
    "        rho, pvalue = stats.spearmanr(sign * df_nna[column], df_nna[target_column])\n",
    "        if sample_conf_interval:\n",
    "            lower, upper, _ = bootstrap_confidence_interval(\n",
    "                sign * df_nna[column].values,\n",
    "                df_nna[target_column].values,\n",
    "                fn=lambda v1, v2: stats.spearmanr(v1, v2)[0],\n",
    "                show_progress=False,\n",
    "            )\n",
    "        else:\n",
    "            lower, upper = compute_spearman_ci(rho, len(df_nna))\n",
    "        corrs[column] = (rho, lower, upper, len(df_nna))\n",
    "    return corrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_spearman_corrs(corrs):\n",
    "    for column, corr in corrs.items():\n",
    "        print(f\"{column:30s} {corr[0]:+.4} {corr[1]:.4} ({corr[2]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import set_matplotlib_formats\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "\n",
    "set_matplotlib_formats(\"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = plt.cm.get_cmap(\"tab20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df[[\"dataset\", \"effect_type\"]].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df[\"dataset\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset, gp in sorted(result_df.groupby([\"dataset\"])):\n",
    "    gp = gp[gp[\"rev\"] == False]\n",
    "    print(dataset, len(gp[\"unique_id\"].unique()), len(gp[[\"unique_id\", \"mutation\"]].drop_duplicates()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset, gp in sorted(input_test_df.groupby([\"dataset\"])):\n",
    "    gp = gp[gp[\"rev\"] == False]\n",
    "    print(dataset, len(gp[\"unique_id\"].unique()), len(gp[[\"unique_id\", \"mutation\"]].drop_duplicates()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev = [False]\n",
    "\n",
    "if rev == [False]:\n",
    "    suffix = \"\"\n",
    "else:\n",
    "    assert rev == [False, True]\n",
    "    suffix = \"-rev\" \n",
    "\n",
    "corrs_dict = {}\n",
    "for idx, (dataset, effect_type, eval_columns) in enumerate(datasets_eval):\n",
    "    print(dataset)\n",
    "\n",
    "    df = result_df[\n",
    "        (result_df[\"effect_type\"] == effect_type)\n",
    "        & (result_df[\"dataset\"] == dataset)\n",
    "        & (result_df[\"rev\"].isin(rev))\n",
    "    ]\n",
    "\n",
    "#     if dataset == \"skempi-v2\":\n",
    "#         df = df[df[\"unique_id\"].isin(skempi_v2_unique_ids)]\n",
    "\n",
    "    corrs = get_spearman_corrs_global(df, eval_columns, \"effect\", sample_conf_interval=False)\n",
    "    corrs_dict[(dataset, effect_type)] = corrs\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = {\n",
    "    \"protherm++\": \"ProTherm\",\n",
    "    \"humsavar\": \"Humsavar\",\n",
    "    \"clinvar\": \"ClinVar\",\n",
    "    \"cosmic\": \"COSMIC\",\n",
    "    \"taipale\": \"Sahni (2015)\",\n",
    "    \"taipale_gpca\": \"Sahni (2015)\",\n",
    "    \"rocklin-2017-core\": \"Rocklin (2017)\",\n",
    "    \"dunham_2020_tianyu\": \"    Dunham (2020)\",\n",
    "    \"skempi++\": \"Skempi\",\n",
    "    \"ab_bind\": \"AB-Bind\",\n",
    "    \"skempi-v2\": \"Skempi v2\",\n",
    "    \"cagi5_frataxin\": \"Savojardo (2019)\",\n",
    "    \"starr_2020_tianyu\": \"Starr (2020)\",\n",
    "    \"huang_2020\": \"Huang (2020)\",\n",
    "}\n",
    "\n",
    "methods = {\n",
    "    \"ddg_pred\": \"EV2\",\n",
    "    \"elaspic_score\": \"ELASPIC\",\n",
    "    \"foldx_score\": \"FoldX\",\n",
    "    \"rosetta_dg_change\": \"Rosetta\",\n",
    "    \"rosetta_complex_dg_change\": \"Rosetta\",\n",
    "    \"provean_score\": \"Provean\",\n",
    "    \"mcsm\": \"mCSM\",\n",
    "    \"popmusic\": \"PoPMuSiC\",\n",
    "    \"proteinsolver_core_score_change\": \"ProteinSolver\",\n",
    "    \"proteinsolver_interface_score_change\": \"ProteinSolver\",\n",
    "    \"protbert_core_score_change\": \"ProtBert\",\n",
    "    \"protbert_interface_score_change\": \"ProtBert\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = {\n",
    "    # Core\n",
    "    (\"elaspic_score\", \"protherm++\", \"core\"): 0.544,\n",
    "    (\"elaspic_score\", \"humsavar\", \"core\"): (0.39825581395348837 + 0.35273972602739727) / 2,\n",
    "    (\"elaspic_score\", \"clinvar\", \"core\"): (0.25872093023255817 + 0.18150684931506849) / 2,\n",
    "    (\"elaspic_score\", \"cosmic\", \"core\"): (0.24418604651162794 + 0.25684931506849313) / 2,\n",
    "    (\"elaspic_score\", \"taipale_gpca\", \"core\"): 0.22093023255813944,\n",
    "    # Interface\n",
    "    (\"elaspic_score\", \"skempi++\", \"interface\"): 0.461,\n",
    "    (\"elaspic_score\", \"humsavar\", \"interface\"): (0.29861111111111116 + 0.32198952879581153) / 2,\n",
    "    (\"elaspic_score\", \"clinvar\", \"interface\"): (0.19444444444444444 + 0.23036649214659692) / 2,\n",
    "    (\"elaspic_score\", \"cosmic\", \"interface\"): (0.15624999999999998 + 0.13350785340314142) / 2,\n",
    "    (\"elaspic_score\", \"ab_bind\", \"interface\"): 0.18062827225130893,\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg, axs = plt.subplots(1, len(datasets_eval), figsize=(9, 3))\n",
    "for idx, (dataset, effect_type, eval_columns) in enumerate(datasets_eval):\n",
    "    corrs = corrs_dict[(dataset, effect_type)].copy()\n",
    "    for key in list(corrs):\n",
    "        if (key, dataset, COI) in scores:\n",
    "            *_, num_rows = corrs[key]\n",
    "            rho = scores[(key, dataset, COI)]\n",
    "            rho_lower, rho_upper = compute_spearman_ci(rho, num_rows)\n",
    "            corrs[key] = (rho, rho_lower, rho_upper)\n",
    "\n",
    "    method_list = list(corrs.keys())\n",
    "    rho_list, rho_lower_list, rho_upper_list, *_ = list(zip(*corrs.values()))\n",
    "    yerr = np.abs(np.c_[rho_lower_list, rho_upper_list].T - np.array(rho_list))\n",
    "\n",
    "    ax = axs[idx]\n",
    "    x = np.arange(len(method_list))\n",
    "    out = ax.bar(x, rho_list, yerr=yerr, width=0.7, capsize=1, error_kw={\"linewidth\": 1}, color=[cmap(7)] + [cmap(1)] * (len(x) - 1), edgecolor=\"k\")\n",
    "    _ = ax.set_xticks(x)\n",
    "    _ = ax.set_xticklabels([methods[m] for m in method_list], rotation=\"vertical\")\n",
    "    ax.set_title(titles[dataset], fontsize=10.5)\n",
    "    ax.set_ylim(0.0, 0.65)\n",
    "    if idx == 0:\n",
    "        ax.set_ylabel(\"Spearman's ρ\")\n",
    "        ax.yaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
    "    else:\n",
    "        ax.set_yticklabels([])\n",
    "\n",
    "fg.subplots_adjust(top=0.9, right=0.97, bottom=0.38, left=0.08, hspace=0.0, wspace=0.15)\n",
    "fg.savefig(NOTEBOOK_DIR.joinpath(f\"corrs-xval-{COI}{suffix}.svg\"), dpi=300)\n",
    "fg.savefig(NOTEBOOK_DIR.joinpath(f\"corrs-xval-{COI}{suffix}.png\"), dpi=300)\n",
    "fg.savefig(NOTEBOOK_DIR.joinpath(f\"corrs-xval-{COI}{suffix}.pdf\"), dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVG(NOTEBOOK_DIR.joinpath(f\"corrs-xval-{COI}{suffix}.svg\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if COI == \"core\":\n",
    "    eval_columns = [\n",
    "        \"ddg_pred\",\n",
    "        \"elaspic_score\",\n",
    "        \"foldx_score\",\n",
    "        \"rosetta_dg_change\",\n",
    "        \"mcsm\",\n",
    "        \"popmusic\",\n",
    "        #\n",
    "#         \"provean_score\",\n",
    "#         \"proteinsolver_core_score_change\",\n",
    "#         \"protbert_core_score_change\",\n",
    "    ]\n",
    "else:\n",
    "    eval_columns = [\n",
    "        \"ddg_pred\",\n",
    "        \"elaspic_score\",\n",
    "        \"foldx_score\",\n",
    "        \"rosetta_complex_dg_change\",\n",
    "#         \"provean_score\",\n",
    "        \"mcsm\",\n",
    "        \"proteinsolver_interface_score_change\",\n",
    "        \"protbert_interface_score_change\"\n",
    "        #\n",
    "#         \"proteinsolver_core_score_change\",\n",
    "#         \"protbert_core_score_change\",\n",
    "        #\n",
    "#         \"rosetta_opt_apart_dg_change\",\n",
    "#         \"rosetta_apart_dg_change\",\n",
    "#         \"rosetta_opt_bind_dg_change\",\n",
    "#         \"rosetta_bind_dg_change\",\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scores = {\n",
    "    # Core\n",
    "    (\"starr_2020_tianyu\", \"elaspic_score\", \"core\"): -0.08604288185511758,\n",
    "    (\"starr_2020_tianyu\", \"foldx_score\", \"core\"): 0.48977200134358584,\n",
    "    (\"starr_2020_tianyu\", \"provean_score\", \"core\"): 0.4314993652741159,\n",
    "    (\"starr_2020_tianyu\", \"mcsm\", \"core\"): 0.3331869564512911,\n",
    "    (\"starr_2020_tianyu\", \"popmusic\", \"core\"): 0.44773818136812926,\n",
    "    # Interface\n",
    "    (\"starr_2020_tianyu\", \"elaspic_score\", \"interface\"): 0.5140238363135885,\n",
    "    (\"starr_2020_tianyu\", \"foldx_score\", \"interface\"): 0.5294542669183915,\n",
    "    (\"starr_2020_tianyu\", \"provean_score\", \"interface\"): 0.4133588948415616,\n",
    "    (\"starr_2020_tianyu\", \"mcsm\", \"interface\"): 0.36531919399161616,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset, effect_type = (\"huang_2020\", \"ΔΔG\")\n",
    "dataset, effect_type = (\"starr_2020_tianyu\", \"Deep mutation scan\")\n",
    "# dataset, effect_type = (\"cagi5_frataxin\", \"ΔΔG\")\n",
    "\n",
    "rev = [False]\n",
    "\n",
    "df = input_test_df[\n",
    "    (input_test_df[\"effect_type\"] == effect_type)\n",
    "    & (input_test_df[\"dataset\"] == dataset)\n",
    "    & (input_test_df[\"rev\"].isin(rev))\n",
    "]\n",
    "\n",
    "idx = 0\n",
    "\n",
    "eval_columns_ = [c for c in eval_columns if (dataset, c, COI) not in test_scores]\n",
    "print(eval_columns_)\n",
    "corrs = get_spearman_corrs_global(df, eval_columns_, \"effect\", sample_conf_interval=False)\n",
    "for column in eval_columns:\n",
    "    if (dataset, column, COI) in test_scores:\n",
    "        *_, num_rows = corrs[\"ddg_pred\"]\n",
    "        rho = test_scores[(dataset, column, COI)]\n",
    "        rho_lower, rho_upper = compute_spearman_ci(rho, num_rows)\n",
    "        corrs[column] = (rho, rho_lower, rho_upper)\n",
    "        print(column, rho)\n",
    "\n",
    "fg, ax = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "rho_list, rho_lower_list, rho_upper_list, *_ = list(zip(*[corrs[c] for c in eval_columns]))\n",
    "yerr = np.abs(np.c_[rho_lower_list, rho_upper_list].T - np.array(rho_list))\n",
    "\n",
    "x = np.arange(len(eval_columns))\n",
    "out = ax.bar(x, rho_list, yerr=yerr, width=0.7, capsize=1, error_kw={\"linewidth\": 1}, color=[cmap(7)] + [cmap(1)] * (len(x) - 1), edgecolor=\"k\")\n",
    "_ = ax.set_xticks(x)\n",
    "_ = ax.set_xticklabels([methods[m] for m in eval_columns], rotation=\"vertical\")\n",
    "ax.set_title(titles[dataset], fontsize=12)\n",
    "ax.set_ylim(0, 0.69)\n",
    "if idx == 0:\n",
    "    ax.set_ylabel(\"Spearman's ρ\")\n",
    "    ax.yaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
    "else:\n",
    "    ax.set_yticklabels([])\n",
    "\n",
    "fg.subplots_adjust(top=0.92, right=0.99, bottom=0.35, left=0.125, hspace=0.0, wspace=0.12)\n",
    "fg.savefig(NOTEBOOK_DIR.joinpath(f\"corrs-test-{COI}.svg\"), dpi=300)\n",
    "fg.savefig(NOTEBOOK_DIR.joinpath(f\"corrs-test-{COI}.png\"), dpi=300)\n",
    "fg.savefig(NOTEBOOK_DIR.joinpath(f\"corrs-test-{COI}.pdf\"), dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVG(NOTEBOOK_DIR.joinpath(f\"corrs-test-{COI}.svg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scores for ELASPIC v1 were extracted from my thesis: <https://ostrokach.gitlab.io/msc_thesis/msc_thesis.pdf>.\n",
    "\n",
    "**Core**\n",
    "\n",
    "*Validation*\n",
    "\n",
    "```\n",
    "Humsavar, 3.9825581395348837\n",
    "ClinVar, 2.587209302325581\n",
    "COSMIC, 2.4418604651162794\n",
    "Taipale, 2.2093023255813944\n",
    "```\n",
    "\n",
    "*Test*\n",
    "\n",
    "```\n",
    "Humsavar, 3.5273972602739727\n",
    "Clinvar, 1.8150684931506849\n",
    "COSMIC, 2.5684931506849313\n",
    "AB-Bind, 4.075342465753424\n",
    "```\n",
    "\n",
    "**Interface**\n",
    "\n",
    "*Validation*\n",
    "\n",
    "```\n",
    "Humsavar, 2.9861111111111116\n",
    "ClinVar, 1.9444444444444444\n",
    "COSMIC, 1.5624999999999998\n",
    "Taipale PPI, 2.8125\n",
    "Taipale GPCA, 3.680555555555556\n",
    "```\n",
    "\n",
    "*Test*\n",
    "\n",
    "```\n",
    "Humsavar, 3.2198952879581153\n",
    "ClinVar, 2.3036649214659692\n",
    "COSMIC, 1.3350785340314142\n",
    "AB-Bind, 1.8062827225130893\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spearman_corrs_perseq(df, feature_columns, target_column, min_gp_size=6, drop_na=True):\n",
    "    if drop_na:\n",
    "        df = df.dropna(subset=feature_columns + [target_column])\n",
    "    results = {c: [] for c in feature_columns}\n",
    "    for _, gp in df.groupby(\"unique_id\"):\n",
    "        if len(gp) < min_gp_size or len(set(gp[target_column])) < 2:\n",
    "            continue\n",
    "        for column in feature_columns:\n",
    "            sign = -1 if any(column.startswith(prefix) for prefix in [\"provean_\", \"protbert_\", \"proteinsolver_\"]) else 1\n",
    "            gp_nna = gp.dropna(subset=[column, target_column])\n",
    "            corr = stats.spearmanr(sign * gp_nna[column], gp_nna[target_column])\n",
    "            results[column].append(corr[0])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_feval(preds, train_data):\n",
    "    labels = train_data.get_label()\n",
    "    groups = train_data.get_group()\n",
    "    \n",
    "    if len(set(preds)) < 2 or len(set(labels)) < 2:\n",
    "        global_corr = 0\n",
    "    else:\n",
    "        global_corr = stats.spearmanr(preds, labels)[0]\n",
    "    \n",
    "    weighted_corr_total = 0\n",
    "    weight_total = 0\n",
    "    start = 0\n",
    "    for group in groups:\n",
    "        stop = start + group\n",
    "        preds_slice = preds[start:stop]\n",
    "        labels_slice = labels[start:stop]\n",
    "        start = stop\n",
    "\n",
    "        weight = math.sqrt(group)\n",
    "        if group < 2:\n",
    "            continue\n",
    "        elif len(set(labels_slice)) < 2:\n",
    "            continue\n",
    "        elif len(set(preds_slice)) < 2:\n",
    "            group_corr = 0\n",
    "        else:\n",
    "            group_corr =  stats.spearmanr(preds_slice, labels_slice)[0]\n",
    "        weighted_corr_total += weight * group_corr\n",
    "        weight_total += weight\n",
    "    assert start == sum(groups)\n",
    "    pergroup_corr = weighted_corr_total / weight_total\n",
    "        \n",
    "    eval_name = \"wavg_spearman_rho\"\n",
    "    # eval_result = (global_corr / pergroup_corr) / 2\n",
    "    eval_result = pergroup_corr\n",
    "    is_higher_better = True\n",
    "    return eval_name, eval_result, is_higher_better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_score(df):\n",
    "    corr_global = stats.spearmanr(df[\"ddg_pred\"], df[\"effect\"])[0]\n",
    "    \n",
    "    perseq_score = 0\n",
    "    perseq_weight = 0\n",
    "    for _, gp in df.groupby(\"unique_id\"):\n",
    "        if len(set(gp[\"effect\"])) < 2:\n",
    "            continue\n",
    "        elif len(set(gp[\"ddg_pred\"])) < 2:\n",
    "            weight = math.sqrt(len(gp))\n",
    "            corr = 0\n",
    "        else:\n",
    "            weight = math.sqrt(len(gp))\n",
    "            corr = stats.spearmanr(gp[\"ddg_pred\"], gp[\"effect\"])[0]\n",
    "        perseq_score += corr * weight\n",
    "        perseq_weight += weight\n",
    "    corr_perseq = perseq_score / perseq_weight\n",
    "    \n",
    "    return (corr_global + corr_perseq) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
